{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn library\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import  DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Library for diagram visualization\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Outline**\n",
    "\n",
    "- [**Data preparation**](#data-preparation)\n",
    "- [**Decision Tree Theory**](#decision-tree)\n",
    "    - [**Categorical Target Variable: Classification**](#categorical-target-variable-classification)\n",
    "    - [**Continuous target Variable: Regression**](#continuous-target-variable-regression)\n",
    "- [**Algorithm for Decision Tree Regressor**](#algorithm-for-decision-tree-regressor)\n",
    "- [**Algorithm for Random Forest**](#algorithm-for-random-forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the explanation about decision tree models, let's prepare the dataset that will be used. For this, I choose to work with a simple dataset for California Housing Prices from [Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices). The goal is to create a regression model for predicting housing prices by using decision tree and random forest.\n",
    "\n",
    "- **Preparation:**\n",
    "    - keep only the records where `ocean_proximity` is either `<1H OCEAN` or `INLAND`.\n",
    "    - Fill missing values with zeros.\n",
    "    - Apply the log transform to `median_house_value` to reduce outlier influence.\n",
    "    - train/validation/test split with 60%/20%/20% distribution.\n",
    "    - Use `DictVectorizer(sparse=True)` to turn the dataframes into matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/housing.csv\")\n",
    "#display(df.head(2))\n",
    "\n",
    "# Keep only  '<1H OCEAN' and 'INLAND' from ocean_proximity\n",
    "df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n",
    "\n",
    "# Fill missing values\n",
    "#print('Missing values\\n',df.isnull().sum()[4:5])\n",
    "df.fillna(0, inplace = True) \n",
    "\n",
    "# Log transform to median_house_value\n",
    "df['median_house_value'] = df['median_house_value'].agg(np.log1p)\n",
    "\n",
    "# Train/Validation/Test split\n",
    "df_train_large, df_test = train_test_split(df, test_size = 0.2, random_state = 1)\n",
    "df_train, df_val = train_test_split(df_train_large, train_size = 0.75, random_state = 1)\n",
    "\n",
    "Y_train_large = df_train_large['median_house_value'].values\n",
    "Y_test = df_test['median_house_value'].values\n",
    "Y_train = df_train['median_house_value'].values\n",
    "Y_val = df_val['median_house_value'].values\n",
    "\n",
    "# Drop target variable from train, validation and test sets\n",
    "df_train_large.drop('median_house_value', axis=1, inplace=True)\n",
    "df_train.drop('median_house_value', axis=1, inplace=True)\n",
    "df_val.drop('median_house_value', axis=1, inplace=True)\n",
    "df_test.drop('median_house_value', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Convert DataFrames to dictionary records\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "train_large_dict = df_train_large.to_dict(orient='records')\n",
    "val_dict = df_val.to_dict(orient='records')\n",
    "test_dict = df_test.to_dict(orient='records')\n",
    "\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Fit and transform\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "X_train_large = dv.transform(train_large_dict)\n",
    "X_val = dv.transform(val_dict)\n",
    "X_test = dv.transform(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a hypothetical dataset with multiples numerical features $\\mathbf{X}_{1},\\cdots, \\mathbf{X}_{d}$ and a target variable $\\mathbf{Y}$. These features and the target variable are represented as column vectors in the dataset, as shown:\n",
    "\n",
    "$$\n",
    "\\left( \\begin{array}{c|ccc|c}\n",
    "\\text{Instance}    &\\mathbf{X}_{1}&\\cdots & \\mathbf{X}_{d}  & \\mathbf{Y}\\\\\n",
    "\\hline\n",
    "\\mathbf{x}_{1} & x_{11}& \\cdots&x_{1d}&y_1 \\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n",
    "\\mathbf{x}_{n}&x_{n1}&\\cdots&x_{nd}&y_n\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "In this representation, each row vector $ \\mathbf{x}_i = ( x_{i1}, \\ldots, x_{id})$ constitutes an instance of the dataset, with $d$ feature values. The index $i$ ranges from 1 to $n$, where $n$ is the total number of instances. The dataset can be further decomposed into a feature matrix $\\mathbf{X}$ and a target vector $\\mathbf{Y}$:\n",
    "\n",
    "$$\\mathbf{X}=\n",
    "\\left( \\begin{array}{ccc}\n",
    "  x_{11}& \\cdots&x_{1d} \\\\\n",
    "\\vdots&\\ddots&\\vdots&\\\\\n",
    "x_{n1}&\\cdots&x_{nd}\n",
    "\\end{array} \\right) ~~~ \\text{and} ~~~ \n",
    "\n",
    "\\mathbf{Y} = \\left( \\begin{array}{c}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Here, the feature matrix $\\mathbf{X}$ is composed of all feature vectors, while the target vector $\\mathbf{Y}$ can take either discrete or continuous values, depending on the problem context. In the case of discrete values, each unique class in $\\mathcal{Y}$ corresponds to a partition of the dataset, formed by grouping instances $\\mathbf{x}_i$ with the same class. For continuous values, the target variable can represent a range of numerical outcomes.\n",
    "\n",
    "A decision tree is a recursive model that employs partition-based methods to predict the class $\\hat{y}_i$ for each instance $\\mathbf{x}_i$. The process starts by splitting the dataset into two partitions. These partitions are further divided recursively, until a state is achieved in which the majority of instances  $\\mathbf{x}_i$ within a partition belong to the same class.\n",
    "\n",
    "One important partition-based method employed in most decision tree models, such as CART, is the axis-parallel hyperplane. This approach is commonly used in high-dimensional spaces represented by the dataset's features. The term \"hyperplane\" refers to the generalization of a geometric plane in a space with more than three dimensions.\n",
    "\n",
    "The mathematical formulation for such a hyperplane is given by the condition:\n",
    "\n",
    "$$h(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b\\leq 0$$\n",
    "\n",
    "Here, the $\\mathbf{x}$ variable in this function $h(\\mathbf{x})$ can be any instance from the dataset. The weight vector $\\mathbf{w}$ is restricted a priori to one of the standard basis vectors $\\{\\mathbf{e}_1,\\cdots,\\mathbf{e}_j,\\cdots \\mathbf{e}_d\\}$, where $\\mathbf{e}_j$ has a value of 1 for the jth dimension and 0 for all other dimensions. This implies that the weights determine the orientation of the hyperplane in one of the basis vector directions, while the bias term translates it along that axis. The base vectors of a $d$-dimensional space are given by:\n",
    "\n",
    "$$ \n",
    "\\mathbf{e}_1 = \\left( \\begin{array}{c}\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array} \\right),~\n",
    "\\mathbf{e}_i = \\left( \\begin{array}{c}\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array} \\right),~~\n",
    "\\mathbf{e}_d = \\left( \\begin{array}{c}\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "1\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "On the other hand, the inequality $h(\\mathbf{x}) \\leq 0$ serves a particular purpose: it defines a half-space. Any instance $\\mathbf{x}$ for which $h(\\mathbf{x}) \\leq 0$ lies on one side of the hyperplane, and any instance for which $h(\\mathbf{x}) > 0$ lies on the other side. In this way, the hyperplane acts as a decision boundary that partitions the dataset into two partitions based on the sign of $h(\\mathbf{x})$.\n",
    "\n",
    "For a given standard basis vector chosen as the weight $\\mathbf{w} = \\mathbf{e}_j$, where $j$ can be an integer between 1 and $d$, the decision condition $h(\\mathbf{x})$ for some instance $\\mathbf{x}_i$ is represented by:\n",
    "\n",
    "$$h(\\mathbf{x}_i) = \\mathbf{e}_j \\cdot \\mathbf{x}_i  + b \\leq 0$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$x_{ij} \\leq t$$\n",
    "\n",
    "Here $t = -b$ represents a specific value within the domain of the feature vector $\\mathbf{X}_j$.The split condition for the ith feature will be then the value of the jth element from the row vector $\\mathbf{X}_j$. The optimal offset $b$ is chosen to minimize a particular criterion, such as a loss function, for the partitioned datasets.\n",
    "\n",
    "For simplicity, let $\\mathcal{D}$ denote the dataset, which also serves as a representation of the feature space for our discussion. Upon applying the decision boundary, the dataset $\\mathcal{D}$ is divided into two mutually exclusive partitions: $\\mathcal{D}_Y$ and $\\mathcal{D}_N$. The partition $\\mathcal{D}_Y$ includes instances that satisfy the inequality $x_{ij} \\leq t$, while $\\mathcal{D}_N$ includes those that do not. More formally, for each instance $\\mathbf{x}_i$:\n",
    "\n",
    "- If the $j$-th feature $x_{ij}$ value for the $i$-th instance satisfies the condition $x_{ij} \\leq t$, then the instance is allocated to the set\n",
    "\n",
    "  $$\\mathcal{D}_Y = \\{\\mathbf{x}_i| x_{ij} \\leq  t\\}$$\n",
    "  \n",
    "  which grouped all instances $\\mathbf{x}_i$ such that their $j$-th feature value $x_{ij}$ is less than or equal to the threshold $t$.\n",
    "\n",
    "- Otherwise, the instance are allocated into the set\n",
    "\n",
    "  $$\\mathcal{D}_N = \\{\\mathbf{x}_i| x_{ij} >  t\\ \\}$$\n",
    "\n",
    "In this context, $t$ is a pre-selected threshold that serves as the decision boundary that separates the two partitions\n",
    "\n",
    "\n",
    "Given that $x_{ij}$ is a element from the feature vector $\\mathbf{X}_j$, we can turn this more explicitly in the notation. To express this condition in a more generic form that represents the behavior across the entire feature, we can write the inequality $x_{ij} \\leq t$ for all $i = 1, 2, \\ldots, n$ as\n",
    "\n",
    "$$\\mathbf{X}_j \\leq t$$\n",
    "\n",
    "This generic form implicitly implies that the condition is applicable to each element $x_{ij}$, where $i = 1, 2, \\ldots, n $ across the entire dataset $\\mathcal{D}$. By using this generic representation, the rules becomes a general principle that applies not just to individual instances, but to the entire feature, providing a view on how the feature $\\mathbf{X}_j$ contributes to the partitioning of $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Particular case in two dimension**\n",
    "\n",
    "To illuminate the partition-based methods used in the decision tree model, let's consider a hypothetical scenario involving a synthetically generated dataset. This dataset will have two features, $\\mathbf{X}_1$ and $\\mathbf{X}_2$, and a multi-class target variable $\\mathbf{Y}$ with four possible classes. Mathematically the dataset can be represented as:\n",
    "\n",
    "$$\n",
    "\\left( \\begin{array}{c|cc|c}\n",
    "\\text{Instance}    &\\mathbf{X}_{1}& \\mathbf{X}_{2}  & \\mathbf{Y}\\\\\n",
    "\\hline\n",
    "\\mathbf{x}_{1} & x_{11}&x_{12}&y_1 \\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "\\mathbf{x}_{n}&x_{n1}&x_{n2}&y_n\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "where each instance will be randomly generated to be in one of the four possible classes of the feature vector $\\mathbf{Y}$. Then two threshold values are selected to partition the dataset $\\mathcal{D}$ into four distinct regions $\\mathcal{D}_1$, $\\mathcal{D}_2$  $\\mathcal{D}_3$ and $\\mathcal{D}_4$ .\n",
    "\n",
    "<center><img src = \"figures/axis_parallel.png\" width=\"500\" height=\"400\"/></center>\n",
    "\n",
    "By observing the plot, we can gain a visual intuition into how partition-based methods would partition the feature space to isolate instances of different classes.  Each region within this space is defined by a set of rules. These rules act as decision conditions, determining the region to which each instance belongs. When the model evaluates a given instance, it follows this set of decision rules:\n",
    "\n",
    "\n",
    "- $\\mathcal{D_1}$: If $\\mathbf{X}_1 \\leq 3.5$ and $\\mathbf{X}_2 > 4.5$, then belongs to  class 4\n",
    "- $\\mathcal{D_2}$: If $\\mathbf{X}_1 > 3.5$ and $\\mathbf{X}_2 > 4.5$, then belongs to  class 2\n",
    "- $\\mathcal{D}_3$: If $\\mathbf{X}_1 \\leq 3.5$ and $\\mathbf{X}_2 \\leq 4.5$, then belongs to  class 1\n",
    "- $\\mathcal{D_4}$: If $\\mathbf{X}_1 > 3.5$ and $\\mathbf{X}_2 \\leq 4.5$, then belongs to  class 3\n",
    "\n",
    "After established the partitioning of our feature space into distinct regions, we can represent these partitions in a more structured form, using decision tree diagram, which gives the name of the model. A decision tree diagram will represent a series of decisions made on the features of the dataset to reach a conclusion about the class of an instance. \n",
    "\n",
    "To those decision rules, we can draw the following diagram which visually can give a interpretation of the decision-making process in classifying an instance based on its feature values. Consider the following diagram of a decision tree:\n",
    "\n",
    "<center><img src = \"figures/decision_tree_diagram.png\" width=\"600\" height=\"400\"/></center>\n",
    "\n",
    "Each node in the decision tree represents a decision criterion based on a feature's threshold, and each branch represents the outcome of the test. The leaves of the tree represent the final classes or regions, $\\mathcal{D_1}, \\mathcal{D_2}, \\mathcal{D_3}$, and $\\mathcal{D_4}$. By the diagram of the tree, we have\n",
    "\n",
    "- The **red** node, which is the root, with the initial question about the feature's threshold.\n",
    "- The **blue** nodes are the decision nodes that further refine the classification by adding more conditions and creating the branch.\n",
    "- The **green** nodes are the leaves of the tree, which is the final stage where each regions has a majority of one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Categorical Target Variable: Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics Evaluation for classification**\n",
    "\n",
    "In classification, our task is to classify instances by their classes from the target variable $\\mathbf{Y}$. A important step in this process is evaluating how well a partition in the feature space distinguishes between these classes. To evaluate the quality of a partition in the feature space, we need a metric evaluation to select an optimal split point. But before entering into the evaluation metrics, it's necessary to understand how probabilities are associated with the chances of an instance being allocated in one partition or another. These probabilities will provide the basis for measuring the class distribution within a partition, which in turn influences the quality of the split.\n",
    "\n",
    "When we have numerical features, one possible strategy is to consider only the midpoints between two successive and distinct values of a given feature $\\mathbf{X}_j$. These midpoints can be denoted as ${t_1, \\cdots, t_m}$, such that $t_1 < \\cdots < t_m$. Each split point $\\mathbf{X}_j\\leq t$ necessitates the definition of a probability mass function (PMF). In the context of decision trees, a PMF will be a function that provides the probabilities of discrete classes occurring within a given partition.\n",
    "\n",
    "The probability mass function for a split point $\\mathbf{X}_j\\leq t$ is:\n",
    "\n",
    "$$p(c_i|\\mathcal{D}_Y) = p(c_i|\\mathbf{X}_j \\leq t) $$\n",
    "\n",
    "$$p(c_i|\\mathcal{D}_N) = p(c_i|\\mathbf{X}_j > t)$$\n",
    "\n",
    "Here, $c_i$ is one of the classes $\\{c_1, \\cdots, c_k \\}$ from the target vector $\\mathbf{Y}$. These PMFs account for the distribution of each class $c_i$ in the partitions $\\mathcal{D}_Y$ and $\\mathcal{D}_N$. In this formulation, $p(c_i|\\mathbf{X}_j \\leq t)$ represents the probability of class $c_i$ being in partition $\\mathcal{D}_Y$, and similarly $p(c_i|\\mathbf{X}_j > t)$ for partition $\\mathcal{D}_N$.\n",
    "\n",
    "The utility of these PMFs is to quantify the idea of \"purity\" for each partition with respect to the distribution of target classes. A partition is considered \"pure\" if it contains instances predominantly from a single class, therefore yielding a higher probability for that class in the corresponding PMF.\n",
    "\n",
    "**Metrics**\n",
    "\n",
    "Evaluating the quality of split points is an important step for optimizing the performance of a decision tree. Different metrics offer various ways to measure the \"purity\" of the partitions created by each split.\n",
    "\n",
    "- **Information Gain**\n",
    "\n",
    "    Information gain measures the reduction of disorder or uncertainty in a system. The goal is to use entropy as a metric for each partition, favoring a lower entropy if the partition is pure (i.e., most instances have the same class label). On the other hand, if class labels are mixed with no majority class, a partition has higher entropy.\n",
    "\n",
    "    The entropy of a set of dataset $\\mathcal{D}$ is defined as:\n",
    "\n",
    "    $$H(\\mathcal{D}) = - \\sum^{k}_{i=1} p(c_i| \\mathcal{D}) \\log{p(c_i| \\mathcal{D})}$$\n",
    "\n",
    "    where $p(c_i| \\mathcal{D})$ is the probability of class $c_i$ in $\\mathcal{D}$, and $k$ is the number of different classes from the target vector $\\mathbf{Y}$.\n",
    "\n",
    "    When a split  point $\\mathbf{X}_j\\leq t$ partitions $\\mathcal{D}$ into $\\mathcal{D}_Y = \\{\\mathbf{x}_i| x_{ij} \\leq  t\\}$ and $\\mathcal{D}_N = \\{\\mathbf{x}_i| x_{ij} >  t\\ \\}$, we define the split entropy as the weighted entropy of each of the resulting partitions:\n",
    "\n",
    "    $$H(\\mathcal{D}_Y, \\mathcal{D}_N) = \\frac{n_Y}{n}H(\\mathcal{D}_Y) + \\frac{n_N}{n}H(\\mathcal{D}_N)$$\n",
    "\n",
    "    where $n = |\\mathcal{D}|$ is the number of instances in $\\mathcal{D}$, and $n_Y = |\\mathcal{D}_Y|$ and $n_N = |\\mathcal{D}_N|$ are the number of points in $\\mathcal{D}_Y$ and $\\mathcal{D}_N$, respectively.\n",
    "\n",
    "    The information gain for a given split point, representing the reduction in overall entropy, is defined as:\n",
    "\n",
    "    $$\\text{Gain}(\\mathcal{D},\\mathcal{D}_Y, \\mathcal{D}_N) = H(\\mathcal{D}) - H(\\mathcal{D}_Y, \\mathcal{D}_N)$$\n",
    "\n",
    "    A higher information gain corresponds to a greater reduction in entropy, thus representing a better split point. Therefore, we can score each split point and select the one that provides the highest information gain.\n",
    "\n",
    "\n",
    "- **Gini Index**\n",
    "\n",
    "    The Gini index, another common purity measure for a split point, is defined as:\n",
    "\n",
    "    $$G(\\mathcal{D}) = 1 - \\sum_{i=1}^{k} p(c_i| \\mathcal{D})^2$$\n",
    "\n",
    "    A higher Gini index value implies less purity or more mixed classes within the partition, while lower values indicate higher purity with respect to the classes. The weighted Gini index of a split point is then defined as:\n",
    "\n",
    "    $$G(\\mathcal{D}_Y, \\mathcal{D}_N) = \\frac{n_Y}{n}G(\\mathcal{D}_Y) + \\frac{n_N}{n}G(\\mathcal{D}_N)$$\n",
    "\n",
    "\n",
    "- **CART**\n",
    "\n",
    "    Another useful measure is the CART, defined as:\n",
    "\n",
    "    $$\\text{CART}(\\mathcal{D}_Y, \\mathcal{D}_N) = 2 \\frac{n_Y}{n}\\frac{n_N}{n} \\sum_{i = 1}^k \\big[p(c_i| \\mathcal{D}_Y )  -p(c_i| \\mathcal{D}_N)\\big]$$\n",
    "\n",
    "    This metric maximizes the difference between the class PMF for the two partitions; a higher CART value implies a better split point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Continuous Target Variable: Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics Evaluation for Regression**\n",
    "\n",
    "In regression, our task is to predict a continuous target variable $\\mathbf{Y}$. When selecting a split point, instead of calculating probabilities over discrete classes, the mean of the target $\\mathbf{Y}$  is calculated for  instances within a given partition. Consequently, each partition is labeled by the mean values of the target variable.\n",
    "\n",
    "\n",
    "Given a feature $\\mathbf{X}_j$ and potential split point $t$, the dataset is divided as follows:\n",
    "\n",
    "- If the feature satisfies the condition $\\mathbf{X}_j \\leq t$, the instance is allocated to the set:\n",
    "\n",
    "    $$\n",
    "    \\mathcal{D}_Y = \\{ \\mathbf{x}_i | \\mathbf{X}_j \\leq t \\}\n",
    "    $$\n",
    "\n",
    "    Here, $\\mathcal{D}_Y$ aggregates all instances $\\mathbf{x}_i$ for which their $j$-th feature $\\mathbf{X}_j$ is less than or equal to the threshold $t$. \n",
    "\n",
    "- Otherwise, the instance is allocated to the set\n",
    "    $$\n",
    "      \\mathcal{D}_N = \\{ \\mathbf{x}_i | \\mathbf{X}_j  > t \\}\n",
    "    $$\n",
    "\n",
    "In both cases, the partitions $\\mathcal{D}_Y$ and $\\mathcal{D}_N$ are labeled by the mean values of $\\mathbf{Y}$ within them. We can calculate the mean of each partition as \n",
    "\n",
    "$$\\mu(\\mathcal{D}_Y) = \\mu_Y = \\frac{\\sum_{i \\in \\mathcal{D}_Y} y_i}{n_Y},~~~\\mu(\\mathcal{D}_N)= \\mu_N = \\frac{\\sum_{i \\in \\mathcal{D}_N} y_i}{n_N}$$\n",
    "\n",
    "where $n_Y = |\\mathcal{D}_Y|$ and $n_N = |\\mathcal{D}_N|$ are are the total number of instances in each set, respectively.\n",
    "\n",
    "After partitioning the dataset into $\\mathcal{D}_Y$ and $\\mathcal{D}_N$, we need a metric for selecting the best split point in regression trees. \n",
    "\n",
    "**Reduction in Variance**\n",
    "\n",
    "The Reduction in Variance metric calculates the decrease in the variance of the target variable $\\mathbf{Y}$ as a result of the split. A **higher reduction in variance** means a more \"informative\" split, leading to subsets that are more homogeneous in terms of the target variable. This is achieved by minimizing the variance within each new partition. Mathematically, the variance of each partition is given by:\n",
    "\n",
    "\n",
    "$$\\sigma^2(\\mathcal{D}_Y) = \\sigma^2_Y=\\frac{1}{n_Y} \\sum_{i \\in \\mathcal{D}_Y} (y_i - \\mu_Y)^2 ~~~\\sigma^2(\\mathcal{D}_N) =\\sigma^2_N= \\frac{1}{n_N} \\sum_{i \\in \\mathcal{D}_N} (y_i - \\mu_N)^2$$\n",
    "\n",
    "The formula for Reduction in Variance, used to compare the variance in the entire dataset $\\mathcal{D}$ with the variances $\\sigma^2_Y$ and $\\sigma^2_N$ in the partitions, is given by:\n",
    "\n",
    "\n",
    "$$\\Delta \\sigma^2 = \\sigma^2(\\mathcal{D}) - \\left( \\frac{n_Y}{n} \\sigma^2(\\mathcal{D}_Y) + \\frac{n_N}{n} \\sigma^2(\\mathcal{D}_N) \\right)$$\n",
    "\n",
    "\n",
    "The objective is to **maximize this reduction** when choosing the best split, which implies minimizing the variance within each new partition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Algorithm for Decision Tree Regressor**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "\n",
    "    def __init__(self, feature_index=None, threshold=None, \n",
    "                 left_node=None, right_node=None, \n",
    "                 var_reduction=None, leaf_value=None, \n",
    "                 node_value = None, samples=0):\n",
    "        \n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold  \n",
    "        # Left and right nodes\n",
    "        self.left_node = left_node\n",
    "        self.right_node = right_node\n",
    "        self.var_reduction = var_reduction\n",
    "        self.node_value = node_value \n",
    "        # leaf node\n",
    "        self.leaf_value = leaf_value\n",
    "        self.samples = samples\n",
    "\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.leaf_value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeRegressor():\n",
    "    def __init__(self, min_samples_split = 2, max_depth = 2):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        dataset = np.concatenate((X, Y.reshape(-1, 1)), axis=1)\n",
    "        self.root = self._grow_tree(dataset)\n",
    "\n",
    "    def _grow_tree(self, dataset, current_depth = 1):\n",
    "        # Recursive function\n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if (self.max_depth is not None) and (current_depth > self.max_depth):\n",
    "                    return Node(leaf_value=self.compute_node(Y),\n",
    "                                 samples=n_samples, node_value=self.compute_node(Y))\n",
    "\n",
    "        if n_samples >= self.min_samples_split:\n",
    "            best_split = self._best_split(X, Y, n_features)\n",
    "            \n",
    "            if (best_split['variance_reduction'] > 0):\n",
    "        \n",
    "                right_subtree = self._grow_tree(best_split['dataset_right'], \n",
    "                                          current_depth + 1)\n",
    "                \n",
    "\n",
    "                left_subtree = self._grow_tree(best_split['dataset_left'], \n",
    "                                         current_depth + 1)\n",
    "            \n",
    "                node = Node(best_split['feature_index'], best_split['threshold'], \n",
    "                    left_subtree, right_subtree, best_split['variance_reduction'],\n",
    "                    samples=n_samples, node_value = self.compute_node(Y)) \n",
    "\n",
    "\n",
    "                return  node   \n",
    "             \n",
    "        # leaf node\n",
    "        return Node(leaf_value=self.compute_node(Y), samples=n_samples, \n",
    "                    node_value = self.compute_node(Y))\n",
    "        \n",
    "        \n",
    "    def _best_split( self, X, Y, n_features):\n",
    "\n",
    "        best_split = {\n",
    "        \"feature_index\": None,\n",
    "        \"threshold\": None,\n",
    "        \"dataset_left\": None,\n",
    "        \"dataset_right\": None,\n",
    "        \"variance_reduction\": -float(\"inf\")\n",
    "        }\n",
    "\n",
    "        # loop over all features\n",
    "        for feature_index in range(n_features):\n",
    "            # Get feature values of all instances\n",
    "            feature_vector = X[:, feature_index]\n",
    "            \n",
    "            # if feature is categorical on-hot encoded\n",
    "            #if set(np.unique(feature_vector)).issubset({0, 1}):\n",
    "            #    thresholds = np.unique(feature_vector)\n",
    "            # if feature is numerical\n",
    "            #else:\n",
    "            sort_features = np.sort(np.unique(feature_vector))\n",
    "            thresholds = (sort_features[:-1] + sort_features[1:]) / 2\n",
    "\n",
    "            # loop over all midpoints and get the best one\n",
    "            for threshold in thresholds:\n",
    "                # Split dataset into two partitions\n",
    "                dataset_left, dataset_right = self._splitter(X, Y, feature_index, threshold)\n",
    "            \n",
    "                # check if partitions are not empty\n",
    "                if (len(dataset_left) > 0 and len(dataset_right) > 0):\n",
    "                    # Get target values of partitions\n",
    "                    Y_left, Y_right = dataset_left[:,-1], dataset_right[:,-1]\n",
    "                    # Calculate variance reduction\n",
    "                    current_reduction = self.variance_reduction(Y, Y_left, Y_right)\n",
    "\n",
    "                    # if the current reduction is better, update the best split \n",
    "                    if current_reduction >  best_split[\"variance_reduction\"] :\n",
    "                        \n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"variance_reduction\"] = current_reduction\n",
    "\n",
    "        return best_split\n",
    "    \n",
    "    def _splitter(self, X, Y, feature_index, threshold):\n",
    "\n",
    "        dataset = np.concatenate((X, Y.reshape(-1, 1)), axis=1)\n",
    "\n",
    "        left_indices = np.argwhere(dataset[:, feature_index] <= threshold).flatten()\n",
    "        right_indices = np.argwhere(dataset[:, feature_index] > threshold).flatten()\n",
    "        \n",
    "        dataset_left = dataset[left_indices]\n",
    "        dataset_right = dataset[right_indices]\n",
    "    \n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "    def variance_reduction(self, Y, Y_left, Y_right):\n",
    "\n",
    "        # Variance of parent and child left/right nodes\n",
    "        varY = np.var(Y)\n",
    "        varY_left = np.var(Y_left)\n",
    "        varY_right = np.var(Y_right)\n",
    "\n",
    "        # weights of child nodes\n",
    "        n, nl, nr = len(Y), len(Y_left), len(Y_right)\n",
    "        w_left = nl / n\n",
    "        w_right = nr / n\n",
    "\n",
    "        return varY - (w_left * varY_left + w_right * varY_right)\n",
    "    \n",
    "    def compute_node(self, Y):\n",
    "        # Get mean of target values\n",
    "        return np.mean(Y)\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        Y_pred = np.array([self._search_tree(instance, self.root) for instance in X])\n",
    "        return Y_pred\n",
    "\n",
    "    def _search_tree(self, instance, node):\n",
    "        # Recursive function\n",
    "        if node.is_leaf:\n",
    "            return node.leaf_value\n",
    "        \n",
    "        # check if instance value is less than node threshold\n",
    "        if instance[node.feature_index] <= node.threshold:\n",
    "            return self._search_tree(instance, node.left_node)\n",
    "        \n",
    "        else:  \n",
    "            return self._search_tree(instance, node.right_node)\n",
    "        \n",
    "    def print_tree(self, node=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "\n",
    "        # Check for leaf nodes\n",
    "        if node.is_leaf:\n",
    "            print(indent, \"Leaf Value:\", node.leaf_value)\n",
    "            print(indent, \"Samples:\", node.samples)\n",
    "            return\n",
    "\n",
    "        # Print information about this node: feature index, threshold, and variance reduction\n",
    "        print(indent, \"Feature X_\" + str(node.feature_index))\n",
    "        print(indent, \"Threshold:\", node.threshold)\n",
    "        print(indent, \"Variance Reduction:\", node.var_reduction)\n",
    "        print(indent, \"Samples:\", node.samples)\n",
    "\n",
    "        # Print the left subtree\n",
    "        print(indent, \"Left:\")\n",
    "        self.print_tree(node.left_node, indent + \"  \")\n",
    "\n",
    "        # Print the right subtree\n",
    "        print(indent, \"Right:\")\n",
    "        self.print_tree(node.right_node, indent + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeVisualizer:\n",
    "    def __init__(self, decision_tree, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.decision_tree = decision_tree\n",
    "        self.dot = Digraph(comment='Decision Tree')\n",
    "        self.node_counter = 0\n",
    "\n",
    "    def add_nodes_edges(self, node, parent_id=None, label=\"\"):\n",
    "        node_id = str(self.node_counter)\n",
    "        self.node_counter += 1\n",
    "        \n",
    "        samples = getattr(node, \"samples\", 0)        \n",
    "        # Display relevant information in the nodes\n",
    "        if node.is_leaf:\n",
    "            display_label = (f'Leaf\\n'\n",
    "                             f'Samples: {samples}\\n'\n",
    "                             f'Value: {node.leaf_value:.2f}')\n",
    "        else:\n",
    "            display_label = (f'{self.feature_names[node.feature_index]} ≤ {node.threshold:.3f}\\n'\n",
    "                             f'Samples: {samples}\\n' \n",
    "                             f'Value: {node.node_value:.2f}')\n",
    "        \n",
    "        self.dot.node(node_id, label=display_label)\n",
    "\n",
    "        if parent_id is not None:\n",
    "            self.dot.edge(parent_id, node_id, label=label)\n",
    "\n",
    "        if not node.is_leaf:\n",
    "            self.add_nodes_edges(node.left_node, node_id, 'Yes')\n",
    "            self.add_nodes_edges(node.right_node, node_id, 'No')\n",
    "\n",
    "    def visualize_tree(self):\n",
    "        self.add_nodes_edges(self.decision_tree.root)\n",
    "        return self.dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**\n",
    "\n",
    "Let's train a decision tree regressor using the scikit-learn library and the algorithm that we created in python to predict the `median_house_value` variable to compare both results. Let's train the models with `max_depth=1`. Which feature is used for splitting the data? The feature used to split the data is the same in both models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"431pt\" height=\"209pt\"\n",
       " viewBox=\"0.00 0.00 430.85 208.91\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 204.91)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-204.91 426.85,-204.91 426.85,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"211.42\" cy=\"-163.43\" rx=\"211.35\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"211.42\" y=\"-174.73\" font-family=\"Times,serif\" font-size=\"14.00\">ocean_proximity=&lt;1H OCEAN ≤ 0.500</text>\n",
       "<text text-anchor=\"middle\" x=\"211.42\" y=\"-159.73\" font-family=\"Times,serif\" font-size=\"14.00\">Samples: 9411</text>\n",
       "<text text-anchor=\"middle\" x=\"211.42\" y=\"-144.73\" font-family=\"Times,serif\" font-size=\"14.00\">Value: 12.01</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"116.42\" cy=\"-37.48\" rx=\"86.03\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"116.42\" y=\"-48.78\" font-family=\"Times,serif\" font-size=\"14.00\">Leaf</text>\n",
       "<text text-anchor=\"middle\" x=\"116.42\" y=\"-33.78\" font-family=\"Times,serif\" font-size=\"14.00\">Samples: 3924</text>\n",
       "<text text-anchor=\"middle\" x=\"116.42\" y=\"-18.78\" font-family=\"Times,serif\" font-size=\"14.00\">Value: 11.61</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M183.69,-126.24C172.88,-112.14 160.4,-95.86 149.15,-81.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"151.91,-79.02 143.05,-73.21 146.35,-83.28 151.91,-79.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"182.92\" y=\"-96.75\" font-family=\"Times,serif\" font-size=\"14.00\">Yes</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"307.42\" cy=\"-37.48\" rx=\"86.03\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"307.42\" y=\"-48.78\" font-family=\"Times,serif\" font-size=\"14.00\">Leaf</text>\n",
       "<text text-anchor=\"middle\" x=\"307.42\" y=\"-33.78\" font-family=\"Times,serif\" font-size=\"14.00\">Samples: 5487</text>\n",
       "<text text-anchor=\"middle\" x=\"307.42\" y=\"-18.78\" font-family=\"Times,serif\" font-size=\"14.00\">Value: 12.30</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M239.45,-126.24C250.38,-112.14 262.98,-95.86 274.35,-81.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.16,-83.26 280.52,-73.21 271.63,-78.98 277.16,-83.26\"/>\n",
       "<text text-anchor=\"middle\" x=\"274.92\" y=\"-96.75\" font-family=\"Times,serif\" font-size=\"14.00\">No</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7a8d6fce4070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 0.454424874819611\n"
     ]
    }
   ],
   "source": [
    "# My Model\n",
    "mytree = MyDecisionTreeRegressor(max_depth = 1)\n",
    "mytree.fit(X_train, Y_train)\n",
    "Y_pred = mytree.predict(X_test)\n",
    "\n",
    "feature_names=dv.feature_names_\n",
    "visualizer = DecisionTreeVisualizer(mytree, feature_names = dv.feature_names_)\n",
    "graph = visualizer.visualize_tree()\n",
    "display(graph)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "print('rmse', rmse)\n",
    "\n",
    "# Running Time : 5.8 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 0.4544248748196092\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scikit-learn Model\n",
    "tree = DecisionTreeRegressor(max_depth = 1)\n",
    "tree.fit(X_train, Y_train)\n",
    "Y_pred = tree.predict(X_test)\n",
    "\n",
    "feature_names=dv.feature_names_\n",
    "plot_tree(tree, feature_names = dv.feature_names_, filled=True)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "print('rmse', rmse)\n",
    "\n",
    "# Running Time : 0.1 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Algorithm for Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A central characteristic of decision tree models is their instability. A minor perturbation in the training dataset can significantly affect the construction of the tree, thereby altering its predictions or decision boundaries. This kind of instability is often observed in systems sensitive to initial conditions, a concept that we can make a parallel in chaos theory. In chaos theory, those systems are central and show dynamic behaviors that are highly dependent on their starting conditions, often referred to as the \"butterfly effect\"\n",
    "\n",
    "This sensitivity is clearly manifested in decision trees. The decisions made at the root node will influence the overall tree structure. A single anomalous instance, like an instance with NaN values or zeros, can drastically influence these initial decisions, leading to a different pattern and predictions. To mitigate this issue, the Random Forest algorithm was introduced.\n",
    "\n",
    "The Random Forest algorithm is an ensemble learning method utilized for both classification and regression tasks. A random forest is simply an ensemble of $k$ decision trees, where each tree is constructed from a slightly perturbed dataset. These datasets are created through bootstrapping, a technique that resamples the original dataset with replacement. Furthermore, each tree is built by sampling a random subset of features at each internal node during the decision tree's construction.\n",
    "\n",
    "The random sampling of features aims to introduce diversity among the trees,  therefore reducing their correlation and improving the ensemble's generalization. By averaging the predictions from the individual trees, the ensemble model more effectively captures the variations in the data, resulting in a more stable and accurate predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRandomForestRegressor():\n",
    "    def __init__(self,  n_estimators= 5, min_samples_split = 2,\n",
    "                  max_depth = None, random_state = 1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        for _ in range(self.n_estimators):\n",
    "            \n",
    "            bootstrap_X, bootstrap_Y = self._bootstrap(X, Y)\n",
    "            tree = MyDecisionTreeRegressor(min_samples_split=self.min_samples_split,\n",
    "                                           max_depth=self.max_depth)\n",
    "            tree.fit(bootstrap_X, bootstrap_Y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _bootstrap(self, X, Y):\n",
    "        n_samples = X.shape[0]\n",
    "        # Create a bootstrap sample\n",
    "        indices = np.random.choice(n_samples, size= n_samples, replace=True, )\n",
    "        return X[indices], Y[indices]\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**\n",
    "\n",
    "Now let's compare both models for the random forest using the following parameters:\n",
    "\n",
    "* `n_estimators=10`\n",
    "* `random_state=1`\n",
    "* `n_jobs=-1` (to make training faster)\n",
    "\n",
    "\n",
    "What's the RMSE on validation? How close are the result of both models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  0.24526091708215395\n"
     ]
    }
   ],
   "source": [
    "# My Model\n",
    "rf = MyRandomForestRegressor(n_estimators = 10, random_state = 1)\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred = rf.predict(X_val)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(Y_val, Y_pred))\n",
    "print('rmse = ', rmse)\n",
    "# Running Time :6m 25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse =  0.244910835217013\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn Model\n",
    "rf = RandomForestRegressor(n_estimators = 10, random_state =1,  n_jobs=-1)\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred = rf.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(Y_val, Y_pred))\n",
    "print('rmse = ', rmse)\n",
    "# Running Time : 0.1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Zaki, M. J., & Meira Jr, W. (2020). Data mining and machine learning: fundamental concepts and algorithms. Cambridge University Press.](https://books.google.com/books/about/Data_Mining_and_Machine_Learning.html?id=oafDDwAAQBAJ&printsec=frontcover&source=kp_read_button&hl=en&newbks=1&newbks_redir=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

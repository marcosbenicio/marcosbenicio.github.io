<!DOCTYPE html>




<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <meta property="og:image" content="/assets/img/post3/decision_tree_diagram.svg"/>
    
    
    <link rel="stylesheet" type="text/css" href="/assets/css/syntax_highlight.css">
    <link rel="stylesheet" type="text/css" href="/assets/css/main.css">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed" href="atom.xml" />

    <!-- MathJax Configuration -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true
          }
        });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    <script>
      function show_tag_section(tag) {
        // Hide all other tag divs
        document.getElementById('all_posts').style.display = 'none';
        var tag_divs = document.getElementsByClassName('by_tag');
        var i;
        for (var i = 0; i < tag_divs.length; i++) {
          tag_divs[i].style.display = 'none';
        }
        // Show the one we want
        document.getElementById(tag).style.display = 'block';
      }
    </script>
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The Theory Behind Tree Based Algorithms | Marcos Benício</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="The Theory Behind Tree Based Algorithms" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A decision tree is a recursive model that employs partition-based methods to predict the class for each instance. The process starts by splitting the dataset into two partitions based on metrics like information gain for classification or variance reduction for regression. These partitions are further divided recursively, until a state is achieved in which most instances within a partition belong to the same class (for classification) or have similar values (for regression). Decision trees also can be extended into ensemble models such as random forests, which combine multiple trees to improve predictive accuracy and reduce overfitting." />
<meta property="og:description" content="A decision tree is a recursive model that employs partition-based methods to predict the class for each instance. The process starts by splitting the dataset into two partitions based on metrics like information gain for classification or variance reduction for regression. These partitions are further divided recursively, until a state is achieved in which most instances within a partition belong to the same class (for classification) or have similar values (for regression). Decision trees also can be extended into ensemble models such as random forests, which combine multiple trees to improve predictive accuracy and reduce overfitting." />
<link rel="canonical" href="http://localhost:4000/2024/04/30/trees.html" />
<meta property="og:url" content="http://localhost:4000/2024/04/30/trees.html" />
<meta property="og:site_name" content="Marcos Benício" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-04-30T00:00:00-03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="The Theory Behind Tree Based Algorithms" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-04-30T00:00:00-03:00","datePublished":"2024-04-30T00:00:00-03:00","description":"A decision tree is a recursive model that employs partition-based methods to predict the class for each instance. The process starts by splitting the dataset into two partitions based on metrics like information gain for classification or variance reduction for regression. These partitions are further divided recursively, until a state is achieved in which most instances within a partition belong to the same class (for classification) or have similar values (for regression). Decision trees also can be extended into ensemble models such as random forests, which combine multiple trees to improve predictive accuracy and reduce overfitting.","headline":"The Theory Behind Tree Based Algorithms","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2024/04/30/trees.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/profile_photo.png"}},"url":"http://localhost:4000/2024/04/30/trees.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body onload="show_tag_section('all_posts')">

    <div class="header">

      <!-- Large header banner on left for large display widths -->
      <div class="big_header">

        <!-- Name -->
        <div class="name_group">
          <h1><a href="/">Marcos Benício</a></h1>
        </div>

        <!-- <a href=""><img src="/assets/img/profile_photo.png" alt="Logo" class="center_img" ></a>-->
        <a href="/"><img src="/assets/img/profile_photo.png" alt="Logo" class="center_img" ></a>
        <!--<a href=""><img src="/assets/img/profile_photo.png" alt="Logo" class="center_img" ></a>-->



        <!-- Position + company -->
        <div class="link_group">

          <div class="row">
              <div class="col1">
                <span> M.sc in Physics</span>
              </div>
          </div>

          <div class="row">
            <!-- <a href=""-->
              <div class="col1">
                <img src="/assets/img/brazil_icon.png" height="30" width="30">
                <!-- <span>  </span>-->
              </div>
            </a>
          </div>

          <div class="row">
            <div class="col1">
              <img src="/assets/img/place_icon_light.png" height="16" width="16">
              <span> Niterói, RJ </span>
            </div>
          </div>

        </div>

        <!-- Badges/links -->
        <div class="link_group">

          <div class="row">
            <a href="https://github.com/marcosbenicio">
              <div class="col2">
                <img src="/assets/img/github_icon_light.png" height="16" width="16">
                <span> Github </span>
              </div>
            </a>
            <a href="https://linkedin.com/in/marcos-benício-de-andrade-alonso-415a5b16b">
              <div class="col2">
                <img src="/assets/img/linkedin_icon_light.png" height="16" width="16">
                <span> LinkedIn </span>
              </div>
            </a>
          </div>

          <div class="row">
            <a href="mailto:marcosbenicio0102@gmail.com">
              <div class="col2">
                <img src="/assets/img/email_icon_light.png" height="16" width="16">
                <span> Email </span>
              </div>
            </a>
            <a href="/assets/files/resume.pdf">
              <div class="col2">
                <img src="/assets/img/cv_icon_light.png" height="16" width="16">
                <span> Resume </span>
              </div>
            </a>
          </div>

        </div>

      </div>

      <!-- Smaller header banner on top for small display widths -->
      <div class="small_header">

        <!-- Name -->
        <div class="small_header_box">
          <div class="name_header_box">
            <div class="name_header_img">
              <a href="/">
                <img src="/assets/img/profile_photo.png" alt="Logo" height="60">
              </a>
            </div>
            <div class="name_header_name">
              <h1><a href="/">Marcos Benício</a></h1>
            </div>
          </div>
        </div>

        <!-- Position + Company -->
        <div class="small_header_box">
          <div class="row">
            <div class="col1">
              <span> M.sc in Physics </span>
            </div>
          </div>
          <div class="row">
            <div class="col1">
             <!-- <a href=""> -->
                  <img src="/assets/img/brazil_icon.png" height="30" width="30">
                  <!-- <span>  </span> -->
              </a>
            </div>
          </div>
        </div>

        <!-- Badges/links -->
        <div class="small_header_box">
                <div class="row">
                  <a href="https://github.com/marcosbenicio">
                    <div class="col2">
                      <img src="/assets/img/github_icon_light.png" height="16" width="16">
                      <span> Github </span>
                    </div>
                  </a>
                  <a href="https://linkedin.com/in/marcos-benício-de-andrade-alonso-415a5b16b">
                    <div class="col2">
                      <img src="/assets/img/linkedin_icon_light.png" height="16" width="16">
                      <span> LinkedIn </span>
                    </div>
                  </a>
                </div>

                <div class="row">
                  <a href="mailto:marcosbenicio0102@gmail.com">
                    <div class="col2">
                      <img src="/assets/img/email_icon_light.png" height="16" width="16">
                      <span> Email </span>
                    </div>
                  </a>
                  <a href="/assets/files/resume.pdf">
                    <div class="col2">
                      <img src="/assets/img/cv_icon_light.png" height="16" width="16">
                      <span> Resume </span>
                    </div>
                  </a>
                </div>
        </div>

      </div>

    </div>

    <!-- Page content -->
    <div class="content">

      <h1>The Theory Behind Tree Based Algorithms</h1>
<p class="meta">30 Apr 2024 - Tags: Decision Tree and Random Forest</p>

<div class="button_container">
  
    <a href="https://github.com/marcosbenicio/marcosbenicio.github.io/blob/main/_posts/notebooks/03-trees/03-trees.ipynb">
      <div class="button_link">
        View on<br /><strong>Github</strong>
      </div>
    </a>
  
  
  
</div>


<div class="post">
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Scikit-learn library
</span><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span>  <span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="n">plot_tree</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="c1"># Library for diagram visualization
</span><span class="kn">from</span> <span class="n">graphviz</span> <span class="kn">import</span> <span class="n">Digraph</span>
</code></pre></div></div>

<h1 id="outline"><strong>Outline</strong></h1>

<ul>
  <li><a href="#data-preparation"><strong>Data preparation</strong></a></li>
  <li><a href="#decision-tree-theory"><strong>Decision Tree Theory</strong></a>
    <ul>
      <li><a href="#categorical-target-variable-classification"><strong>Categorical Target Variable: Classification</strong></a></li>
      <li><a href="#continuous-target-variable-regression"><strong>Continuous target Variable: Regression</strong></a></li>
    </ul>
  </li>
  <li><a href="#algorithm-for-decision-tree-regressor"><strong>Algorithm for Decision Tree Regressor</strong></a></li>
  <li><a href="#algorithm-for-random-forest"><strong>Algorithm for Random Forest</strong></a></li>
</ul>

<h1 id="data-preparation"><strong>Data preparation</strong></h1>

<p>Before starting the explanation about decision tree models, let’s prepare the dataset that will be used. For this, I choose to work with a simple dataset for California Housing Prices from <a href="https://www.kaggle.com/datasets/camnugent/california-housing-prices">Kaggle</a>. The goal is to create a regression model for predicting housing prices by using decision tree and random forest.</p>

<ul>
  <li><strong>Preparation:</strong>
    <ul>
      <li>keep only the records where <code class="language-plaintext highlighter-rouge">ocean_proximity</code> is either <code class="language-plaintext highlighter-rouge">&lt;1H OCEAN</code> or <code class="language-plaintext highlighter-rouge">INLAND</code>.</li>
      <li>Fill missing values with zeros.</li>
      <li>Apply the log transform to <code class="language-plaintext highlighter-rouge">median_house_value</code> to reduce outlier influence.</li>
      <li>train/validation/test split with 60%/20%/20% distribution.</li>
      <li>Use <code class="language-plaintext highlighter-rouge">DictVectorizer(sparse=True)</code> to turn the dataframes into matrices.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">data/housing.csv</span><span class="sh">"</span><span class="p">)</span>
<span class="c1">#display(df.head(2))
</span>
<span class="c1"># Keep only  '&lt;1H OCEAN' and 'INLAND' from ocean_proximity
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">ocean_proximity</span><span class="sh">'</span><span class="p">].</span><span class="nf">isin</span><span class="p">([</span><span class="sh">'</span><span class="s">&lt;1H OCEAN</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">INLAND</span><span class="sh">'</span><span class="p">])]</span>

<span class="c1"># Fill missing values
#print('Missing values\n',df.isnull().sum()[4:5])
</span><span class="n">df</span><span class="p">.</span><span class="nf">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span> 

<span class="c1"># Log transform to median_house_value
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">median_house_value</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">median_house_value</span><span class="sh">'</span><span class="p">].</span><span class="nf">agg</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log1p</span><span class="p">)</span>

<span class="c1"># Train/Validation/Test split
</span><span class="n">df_train_large</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">df_train</span><span class="p">,</span> <span class="n">df_val</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">df_train_large</span><span class="p">,</span> <span class="n">train_size</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">Y_train_large</span> <span class="o">=</span> <span class="n">df_train_large</span><span class="p">[</span><span class="sh">'</span><span class="s">median_house_value</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="sh">'</span><span class="s">median_house_value</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="sh">'</span><span class="s">median_house_value</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>
<span class="n">Y_val</span> <span class="o">=</span> <span class="n">df_val</span><span class="p">[</span><span class="sh">'</span><span class="s">median_house_value</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>

<span class="c1"># Drop target variable from train, validation and test sets
</span><span class="n">df_train_large</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">median_house_value</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_train</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">median_house_value</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_val</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">median_house_value</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_test</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">median_house_value</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>


<span class="c1"># Convert DataFrames to dictionary records
</span><span class="n">train_dict</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">.</span><span class="nf">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="sh">'</span><span class="s">records</span><span class="sh">'</span><span class="p">)</span>
<span class="n">train_large_dict</span> <span class="o">=</span> <span class="n">df_train_large</span><span class="p">.</span><span class="nf">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="sh">'</span><span class="s">records</span><span class="sh">'</span><span class="p">)</span>
<span class="n">val_dict</span> <span class="o">=</span> <span class="n">df_val</span><span class="p">.</span><span class="nf">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="sh">'</span><span class="s">records</span><span class="sh">'</span><span class="p">)</span>
<span class="n">test_dict</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">.</span><span class="nf">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="sh">'</span><span class="s">records</span><span class="sh">'</span><span class="p">)</span>

<span class="n">dv</span> <span class="o">=</span> <span class="nc">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Fit and transform
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">dv</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">train_dict</span><span class="p">)</span>
<span class="n">X_train_large</span> <span class="o">=</span> <span class="n">dv</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">train_large_dict</span><span class="p">)</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">dv</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">val_dict</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">dv</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">test_dict</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="decision-tree-theory"><strong>Decision Tree Theory</strong></h1>

<p>Consider a hypothetical dataset with multiples numerical features  $\mathbf{X_{1}}, \cdots, \mathbf{X_{d}}$  and a target variable  $\mathbf{Y}$ . These features and the target variable are represented as column vectors in the dataset, as shown:</p>

\[\left( \begin{array}{c\midccc\midc}
\text{Instance}    &amp;\mathbf{X}_{1}&amp;\cdots &amp; \mathbf{X}_{d}  &amp; \mathbf{Y}\\
\hline
\mathbf{x}_{1} &amp; x_{11}&amp; \cdots&amp;x_{1d}&amp;y_1 \\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\
\mathbf{x}_{n}&amp;x_{n1}&amp;\cdots&amp;x_{nd}&amp;y_n
\end{array} \right).\]

<p>In this representation, each row vector $\mathbf{x_i} = ( x_{i1}, \ldots, x_{id})$ constitutes an instance of the dataset, with $d$ feature values. The index $i$ ranges from 1 to $n$, where $n$ is the total number of instances. The dataset can be further decomposed into a feature matrix $\mathbf{X}$ and a target vector $\mathbf{Y}$:</p>

\[\mathbf{X}=
\left( \begin{array}{ccc}
  x_{11}&amp; \cdots&amp;x_{1d} \\
\vdots&amp;\ddots&amp;\vdots&amp;\\
x_{n1}&amp;\cdots&amp;x_{nd}
\end{array} \right) ~~~ \text{and} ~~~ 

\mathbf{Y} = \left( \begin{array}{c}
y_1\\
\vdots\\
y_n
\end{array} \right)\]

<p>Here, the feature matrix $\mathbf{X}$ is composed of all feature vectors, while the target vector $\mathbf{Y}$ can take either discrete or continuous values, depending on the problem context. In the case of discrete values, each unique class in $\mathcal{Y}$ corresponds to a partition of the dataset, formed by grouping instances $\mathbf{x_i}$ with the same class. For continuous values, the target variable can represent a range of numerical outcomes.</p>

<p>A decision tree is a recursive model that employs partition-based methods to predict the class $\hat{y}_i$ for each instance $\mathbf{x_i}$. The process starts by splitting the dataset into two partitions. These partitions are further divided recursively, until a state is achieved in which the majority of instances  $\mathbf{x_i}$ within a partition belong to the same class.</p>

<p>One important partition-based method employed in most decision tree models, such as CART, is the axis-parallel hyperplane. This approach is commonly used in high-dimensional spaces represented by the dataset’s features. The term “hyperplane” refers to the generalization of a geometric plane in a space with more than three dimensions.</p>

<p>The mathematical formulation for such a hyperplane is given by the condition:</p>

\[h(\mathbf{x}) = \mathbf{x} \cdot \mathbf{w} + b\leq 0\]

<p>Here, the $\mathbf{x}$ variable in this function $h(\mathbf{x})$ can be any instance from the dataset. The weight vector $\mathbf{w}$ is restricted a priori to one of the standard basis vectors ${\mathbf{e}_1,\cdots,\mathbf{e}_j,\cdots \mathbf{e}_d}$, where $\mathbf{e}_j$ has a value of 1 for the jth dimension and 0 for all other dimensions. This implies that the weights determine the orientation of the hyperplane in one of the basis vector directions, while the bias term translates it along that axis. The base vectors of a $d$-dimensional space are given by:</p>

\[\mathbf{e}_1 = \left( \begin{array}{c}
1\\
\vdots\\
0\\
\vdots\\
0
\end{array} \right),~
\mathbf{e}_i = \left( \begin{array}{c}
0\\
\vdots\\
1\\
\vdots\\
0
\end{array} \right),~~
\mathbf{e}_d = \left( \begin{array}{c}
0\\
\vdots\\
0\\
\vdots\\
1
\end{array} \right)\]

<p>On the other hand, the inequality $h(\mathbf{x}) \leq 0$ serves a particular purpose: it defines a half-space. Any instance $\mathbf{x}$ for which $h(\mathbf{x}) \leq 0$ lies on one side of the hyperplane, and any instance for which $h(\mathbf{x}) &gt; 0$ lies on the other side. In this way, the hyperplane acts as a decision boundary that partitions the dataset into two partitions based on the sign of $h(\mathbf{x})$.</p>

<p>For a given standard basis vector chosen as the weight $\mathbf{w} = \mathbf{e}_j$, where $j$ can be an integer between 1 and $d$, the decision condition $h(\mathbf{x})$ for some instance $\mathbf{x_i}$ is represented by:</p>

\[h(\mathbf{x_i}) = \mathbf{e}_j \cdot \mathbf{x_i}  + b \leq 0\]

<p>which simplifies to:</p>

\[x_{ij} \leq t\]

<p>Here $t = -b$ represents a specific value within the domain of the feature vector $\mathbf{X_j}$.The split condition for the ith feature will be then the value of the jth element from the row vector $\mathbf{X_j}$. The optimal offset $b$ is chosen to minimize a particular criterion, such as a loss function, for the partitioned datasets.</p>

<p>For simplicity, let $\mathcal{D}$ denote the dataset, which also serves as a representation of the feature space for our discussion. Upon applying the decision boundary, the dataset $\mathcal{D}$ is divided into two mutually exclusive partitions: $\mathcal{D_Y}$ and $\mathcal{D_N}$. The partition $\mathcal{D_Y}$ includes instances that satisfy the inequality $x_{ij} \leq t$, while $\mathcal{D_N}$ includes those that do not. More formally, for each instance $\mathbf{x_i}$:</p>

<ul>
  <li>
    <p>If the $j$-th feature $x_{ij}$ value for the $i$-th instance satisfies the condition $x_{ij} \leq t$, then the instance is allocated to the set</p>

\[\mathcal{D_Y} = \{\mathbf{x_i}\mid x_{ij} \leq  t\}\]

    <p>which grouped all instances $\mathbf{x_i}$ such that their $j$-th feature value $x_{ij}$ is less than or equal to the threshold $t$.</p>
  </li>
  <li>
    <p>Otherwise, the instance are allocated into the set</p>

\[\mathcal{D_N} = \{\mathbf{x_i}\mid x_{ij} &gt;  t \}\]
  </li>
</ul>

<p>In this context, $t$ is a pre-selected threshold that serves as the decision boundary that separates the two partitions</p>

<p>Given that $x_{ij}$ is a element from the feature vector $\mathbf{X_j}$, we can turn this more explicitly in the notation. To express this condition in a more generic form that represents the behavior across the entire feature, we can write the inequality $x_{ij} \leq t$ for all $i = 1, 2, \ldots, n$ as</p>

\[\mathbf{X_j} \leq t\]

<p>This generic form implicitly implies that the condition is applicable to each element $x_{ij}$, where $i = 1, 2, \ldots, n $ across the entire dataset $\mathcal{D}$. By using this generic representation, the rules becomes a general principle that applies not just to individual instances, but to the entire feature, providing a view on how the feature $\mathbf{X_j}$ contributes to the partitioning of $\mathcal{D}$.</p>

<p><strong>Particular case in two dimension</strong></p>

<p>To illuminate the partition-based methods used in the decision tree model, let’s consider a hypothetical scenario involving a synthetically generated dataset. This dataset will have two features, $\mathbf{X_1}$ and $\mathbf{X_2}$, and a multi-class target variable $\mathbf{Y}$ with four possible classes. Mathematically the dataset can be represented as:</p>

\[\left( \begin{array}{c\midcc\midc}
\text{Instance}    &amp;\mathbf{X}_{1}&amp; \mathbf{X}_{2}  &amp; \mathbf{Y}\\
\hline
\mathbf{x}_{1} &amp; x_{11}&amp;x_{12}&amp;y_1 \\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
\mathbf{x}_{n}&amp;x_{n1}&amp;x_{n2}&amp;y_n
\end{array} \right).\]

<p>where each instance will be randomly generated to be in one of the four possible classes of the feature vector $\mathbf{Y}$. Then two threshold values are selected to partition the dataset $\mathcal{D}$ into four distinct regions $\mathcal{D_1}$, $\mathcal{D_2}$, $\mathcal{D_3}$ and $\mathcal{D_4}$ .</p>

<center>
<img src="/assets/img/post3/axis_parallel.png" width="500" height="400" />
</center>

<p>By observing the plot, we can gain a visual intuition into how partition-based methods would partition the feature space to isolate instances of different classes.  Each region within this space is defined by a set of rules. These rules act as decision conditions, determining the region to which each instance belongs. When the model evaluates a given instance, it follows this set of decision rules:</p>

<ul>
  <li>
    <p>$\mathcal{D_1}$: If $\mathbf{X_1} \leq 3.5$ and $\mathbf{X_2} &gt; 4.5$, then belongs to class 4.</p>
  </li>
  <li>
    <p>$\mathcal{D_2}$: If $\mathbf{X_1} &gt; 3.5$ and $\mathbf{X_2} &gt; 4.5$, then belongs to class 2.</p>
  </li>
  <li>
    <p>$\mathcal{D_3}$: If $\mathbf{X_1} \leq 3.5$ and $\mathbf{X_2} \leq 4.5$, then belongs to class 1.</p>
  </li>
  <li>
    <p>$\mathcal{D_4}$: If $\mathbf{X_1} &gt; 3.5$ and $\mathbf{X_2} \leq 4.5$, then belongs to class 3.</p>
  </li>
</ul>

<p>After established the partitioning of our feature space into distinct regions, we can represent these partitions in a more structured form, using decision tree diagram, which gives the name of the model. A decision tree diagram will represent a series of decisions made on the features of the dataset to reach a conclusion about the class of an instance.</p>

<p>To those decision rules, we can draw the following diagram which visually can give a interpretation of the decision-making process in classifying an instance based on its feature values. Consider the following diagram of a decision tree:</p>

<center>
<img src="/assets/img/post3/decision_tree_diagram.png" width="600" height="400" />
</center>

<p>Each node in the decision tree represents a decision criterion based on a feature’s threshold, and each branch represents the outcome of the test. The leaves of the tree represent the final classes or regions, $\mathcal{D_1}, \mathcal{D_2}, \mathcal{D_3}$, and $\mathcal{D_4}$. By the diagram of the tree, we have</p>

<ul>
  <li>
    <p>The <strong>red node</strong>, which is the root, with the initial question about the feature’s threshold.</p>
  </li>
  <li>
    <p>The <strong>blue nodes</strong> are the decision nodes that further refine the classification by adding more conditions and creating the branch.</p>
  </li>
  <li>
    <p>The <strong>green nodes</strong> are the leaves of the tree, which is the final stage where each regions has a majority of one class.</p>
  </li>
</ul>

<h2 id="categorical-target-variable-classification"><strong>Categorical Target Variable: Classification</strong></h2>

<p><strong>Metrics Evaluation for classification</strong></p>

<p>In classification, our task is to classify instances by their classes from the target variable $\mathbf{Y}$. A important step in this process is evaluating how well a partition in the feature space distinguishes between these classes. To evaluate the quality of a partition in the feature space, we need a metric evaluation to select an optimal split point. But before entering into the evaluation metrics, it’s necessary to understand how probabilities are associated with the chances of an instance being allocated in one partition or another. These probabilities will provide the basis for measuring the class distribution within a partition, which in turn influences the quality of the split.</p>

<p>When we have numerical features, one possible strategy is to consider only the midpoints between two successive and distinct values of a given feature $\mathbf{X_j}$. These midpoints can be denoted as ${t_1, \cdots, t_m}$, such that $t_1 &lt; \cdots &lt; t_m$. Each split point $\mathbf{X_j}\leq t$ necessitates the definition of a probability mass function (PMF). In the context of decision trees, a PMF will be a function that provides the probabilities of discrete classes occurring within a given partition.</p>

<p>The probability mass function for a split point $\mathbf{X_j}\leq t$ is:</p>

\[p(c_i\mid\mathcal{D_Y}) = p(c_i\mid\mathbf{X_j} \leq t)\]

\[p(c_i\mid\mathcal{D_N}) = p(c_i\mid\mathbf{X_j} &gt; t)\]

<p>Here, $c_i$ is one of the classes ${ c_1, \cdots, c_k }$ from the target vector $\mathbf{Y}$. These PMFs account for the distribution of each class $c_i$ in the partitions $\mathcal{D_Y}$ and $\mathcal{D_N}$. In this formulation, $p(c_i\mid\mathbf{X_j} \leq t)$ represents the probability of class $c_i$ being in partition $\mathcal{D_Y}$, and similarly $p(c_i\mid\mathbf{X_j} &gt; t)$ for partition $\mathcal{D_N}$.</p>

<p>The utility of these PMFs is to quantify the idea of “purity” for each partition with respect to the distribution of target classes. A partition is considered “pure” if it contains instances predominantly from a single class, therefore yielding a higher probability for that class in the corresponding PMF.</p>

<p><strong>Metrics</strong></p>

<p>Evaluating the quality of split points is an important step for optimizing the performance of a decision tree. Different metrics offer various ways to measure the “purity” of the partitions created by each split.</p>

<ul>
  <li>
    <p><strong>Information Gain</strong></p>

    <p>Information gain measures the reduction of disorder or uncertainty in a system. The goal is to use entropy as a metric for each partition, favoring a lower entropy if the partition is pure (i.e., most instances have the same class label). On the other hand, if class labels are mixed with no majority class, a partition has higher entropy.</p>

    <p>The entropy of a set of dataset $\mathcal{D}$ is defined as:</p>

\[H(\mathcal{D}) = - \sum^{k}_{i=1} p(c_i\mid \mathcal{D}) \log{p(c_i\mid \mathcal{D})}\]

    <p>where $p(c_i\mid \mathcal{D})$ is the probability of class $c_i$ in $\mathcal{D}$, and $k$ is the number of different classes from the target vector $\mathbf{Y}$.</p>

    <p>When a split  point $\mathbf{X_j}\leq t$ partitions $\mathcal{D}$ into $\mathcal{D_Y} = {\mathbf{x_i}\mid x_{ij} \leq  t}$ and $\mathcal{D_N} = {\mathbf{x_i}\mid x_{ij} &gt;  t\ }$, we define the split entropy as the weighted entropy of each of the resulting partitions:</p>

\[H(\mathcal{D_Y}, \mathcal{D_N}) = \frac{n_Y}{n}H(\mathcal{D_Y}) + \frac{n_N}{n}H(\mathcal{D_N})\]

    <p>where $n = \mid\mathcal{D}\mid$ is the number of instances in $\mathcal{D}$, and $n_Y = \mid\mathcal{D_Y}\mid$ and $n_N = \mid\mathcal{D_N}\mid$ are the number of points in $\mathcal{D_Y}$ and $\mathcal{D_N}$, respectively.</p>

    <p>The information gain for a given split point, representing the reduction in overall entropy, is defined as:</p>

\[\text{Gain}(\mathcal{D},\mathcal{D_Y}, \mathcal{D_N}) = H(\mathcal{D}) - H(\mathcal{D_Y}, \mathcal{D_N})\]

    <p>A higher information gain corresponds to a greater reduction in entropy, thus representing a better split point. Therefore, we can score each split point and select the one that provides the highest information gain.</p>
  </li>
  <li>
    <p><strong>Gini Index</strong></p>

    <p>The Gini index, another common purity measure for a split point, is defined as:</p>

\[G(\mathcal{D}) = 1 - \sum_{i=1}^{k} p(c_i\mid \mathcal{D})^2\]

    <p>A higher Gini index value implies less purity or more mixed classes within the partition, while lower values indicate higher purity with respect to the classes. The weighted Gini index of a split point is then defined as:</p>

\[G(\mathcal{D_Y}, \mathcal{D_N}) = \frac{n_Y}{n}G(\mathcal{D_Y}) + \frac{n_N}{n}G(\mathcal{D_N})\]
  </li>
  <li>
    <p><strong>CART</strong></p>

    <p>Another useful measure is the CART, defined as:</p>

\[\text{CART}(\mathcal{D_Y}, \mathcal{D_N}) = 2 \frac{n_Y}{n}\frac{n_N}{n} \sum_{i = 1}^k \big[p(c_i\mid \mathcal{D_Y} )  -p(c_i\mid \mathcal{D_N})\big]\]

    <p>This metric maximizes the difference between the class PMF for the two partitions; a higher CART value implies a better split point.</p>
  </li>
</ul>

<h2 id="continuous-target-variable-regression"><strong>Continuous Target Variable: Regression</strong></h2>

<p><strong>Metrics Evaluation for Regression</strong></p>

<p>In regression, our task is to predict a continuous target variable $\mathbf{Y}$. When selecting a split point, instead of calculating probabilities over discrete classes, the mean of the target $\mathbf{Y}$  is calculated for  instances within a given partition. Consequently, each partition is labeled by the mean values of the target variable.</p>

<p>Given a feature $\mathbf{X_j}$ and potential split point $t$, the dataset is divided as follows:</p>

<ul>
  <li>
    <p>If the feature satisfies the condition $\mathbf{X_j} \leq t$, the instance is allocated to the set:</p>

\[\mathcal{D_Y} = \{ \mathbf{x_i} \mid \mathbf{X_j} \leq t \}\]

    <p>Here, $\mathcal{D_Y}$ aggregates all instances $\mathbf{x_i}$ for which their $j$-th feature $\mathbf{X_j}$ is less than or equal to the threshold $t$.</p>
  </li>
  <li>
    <p>Otherwise, the instance is allocated to the set
  \(\mathcal{D_N} = \{ \mathbf{x_i} \mid \mathbf{X_j}  &gt; t \}\)</p>
  </li>
</ul>

<p>In both cases, the partitions $\mathcal{D_Y}$ and $\mathcal{D_N}$ are labeled by the mean values of $\mathbf{Y}$ within them. We can calculate the mean of each partition as</p>

\[\mu(\mathcal{D_Y}) = \mu_Y = \frac{\sum_{i \in \mathcal{D_Y}} y_i}{n_Y},~~~\mu(\mathcal{D_N})= \mu_N = \frac{\sum_{i \in \mathcal{D_N}} y_i}{n_N}\]

<p>where $n_Y = \mid\mathcal{D_Y}\mid$ and $n_N = \mid\mathcal{D_N}\mid$ are are the total number of instances in each set, respectively.</p>

<p>After partitioning the dataset into $\mathcal{D_Y}$ and $\mathcal{D_N}$, we need a metric for selecting the best split point in regression trees.</p>

<p><strong>Reduction in Variance</strong></p>

<p>The Reduction in Variance metric calculates the decrease in the variance of the target variable $\mathbf{Y}$ as a result of the split. A <strong>higher reduction in variance</strong> means a more “informative” split, leading to subsets that are more homogeneous in terms of the target variable. This is achieved by minimizing the variance within each new partition. Mathematically, the variance of each partition is given by:</p>

\[\sigma^2(\mathcal{D_Y}) = \sigma^2_Y=\frac{1}{n_Y} \sum_{i \in \mathcal{D_Y}} (y_i - \mu_Y)^2 ~~~\sigma^2(\mathcal{D_N}) =\sigma^2_N= \frac{1}{n_N} \sum_{i \in \mathcal{D_N}} (y_i - \mu_N)^2\]

<p>The formula for Reduction in Variance, used to compare the variance in the entire dataset $\mathcal{D}$ with the variances $\sigma^2_Y$ and $\sigma^2_N$ in the partitions, is given by:</p>

\[\Delta \sigma^2 = \sigma^2(\mathcal{D}) - \left( \frac{n_Y}{n} \sigma^2(\mathcal{D_Y}) + \frac{n_N}{n} \sigma^2(\mathcal{D_N}) \right)\]

<p>The objective is to <strong>maximize this reduction</strong> when choosing the best split, which implies minimizing the variance within each new partition.</p>

<h1 id="algorithm-for-decision-tree-regressor"><strong>Algorithm for Decision Tree Regressor</strong></h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Node</span><span class="p">():</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">feature_index</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                 <span class="n">left_node</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">right_node</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                 <span class="n">var_reduction</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">leaf_value</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                 <span class="n">node_value</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">feature_index</span> <span class="o">=</span> <span class="n">feature_index</span>
        <span class="n">self</span><span class="p">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>  
        <span class="c1"># Left and right nodes
</span>        <span class="n">self</span><span class="p">.</span><span class="n">left_node</span> <span class="o">=</span> <span class="n">left_node</span>
        <span class="n">self</span><span class="p">.</span><span class="n">right_node</span> <span class="o">=</span> <span class="n">right_node</span>
        <span class="n">self</span><span class="p">.</span><span class="n">var_reduction</span> <span class="o">=</span> <span class="n">var_reduction</span>
        <span class="n">self</span><span class="p">.</span><span class="n">node_value</span> <span class="o">=</span> <span class="n">node_value</span> 
        <span class="c1"># leaf node
</span>        <span class="n">self</span><span class="p">.</span><span class="n">leaf_value</span> <span class="o">=</span> <span class="n">leaf_value</span>
        <span class="n">self</span><span class="p">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_leaf</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">leaf_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>

<span class="k">class</span> <span class="nc">MyDecisionTreeRegressor</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">min_samples_split</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">root</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="n">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">root</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_grow_tree</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_grow_tree</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">current_depth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Recursive function
</span>        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>

        <span class="nf">if </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">current_depth</span> <span class="o">&gt;</span> <span class="n">self</span><span class="p">.</span><span class="n">max_depth</span><span class="p">):</span>
                    <span class="k">return</span> <span class="nc">Node</span><span class="p">(</span><span class="n">leaf_value</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_node</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span>
                                 <span class="n">samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">node_value</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_node</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">n_samples</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">min_samples_split</span><span class="p">:</span>
            <span class="n">best_split</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_best_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
            
            <span class="nf">if </span><span class="p">(</span><span class="n">best_split</span><span class="p">[</span><span class="sh">'</span><span class="s">variance_reduction</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        
                <span class="n">right_subtree</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_grow_tree</span><span class="p">(</span><span class="n">best_split</span><span class="p">[</span><span class="sh">'</span><span class="s">dataset_right</span><span class="sh">'</span><span class="p">],</span> 
                                          <span class="n">current_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                

                <span class="n">left_subtree</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_grow_tree</span><span class="p">(</span><span class="n">best_split</span><span class="p">[</span><span class="sh">'</span><span class="s">dataset_left</span><span class="sh">'</span><span class="p">],</span> 
                                         <span class="n">current_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            
                <span class="n">node</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="n">best_split</span><span class="p">[</span><span class="sh">'</span><span class="s">feature_index</span><span class="sh">'</span><span class="p">],</span> <span class="n">best_split</span><span class="p">[</span><span class="sh">'</span><span class="s">threshold</span><span class="sh">'</span><span class="p">],</span> 
                    <span class="n">left_subtree</span><span class="p">,</span> <span class="n">right_subtree</span><span class="p">,</span> <span class="n">best_split</span><span class="p">[</span><span class="sh">'</span><span class="s">variance_reduction</span><span class="sh">'</span><span class="p">],</span>
                    <span class="n">samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">node_value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_node</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span> 


                <span class="k">return</span>  <span class="n">node</span>   
             
        <span class="c1"># leaf node
</span>        <span class="k">return</span> <span class="nc">Node</span><span class="p">(</span><span class="n">leaf_value</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_node</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="n">samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> 
                    <span class="n">node_value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_node</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
        
        
    <span class="k">def</span> <span class="nf">_best_split</span><span class="p">(</span> <span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_features</span><span class="p">):</span>

        <span class="n">best_split</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">feature_index</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">threshold</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">dataset_left</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">dataset_right</span><span class="sh">"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">variance_reduction</span><span class="sh">"</span><span class="p">:</span> <span class="o">-</span><span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">inf</span><span class="sh">"</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="c1"># loop over all features
</span>        <span class="k">for</span> <span class="n">feature_index</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">):</span>
            <span class="c1"># Get feature values of all instances
</span>            <span class="n">feature_vector</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature_index</span><span class="p">]</span>
            
            <span class="c1"># if feature is categorical on-hot encoded
</span>            <span class="c1">#if set(np.unique(feature_vector)).issubset({0, 1}):
</span>            <span class="c1">#    thresholds = np.unique(feature_vector)
</span>            <span class="c1"># if feature is numerical
</span>            <span class="c1">#else:
</span>            <span class="n">sort_features</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">feature_vector</span><span class="p">))</span>
            <span class="n">thresholds</span> <span class="o">=</span> <span class="p">(</span><span class="n">sort_features</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">sort_features</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">2</span>

            <span class="c1"># loop over all midpoints and get the best one
</span>            <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
                <span class="c1"># Split dataset into two partitions
</span>                <span class="n">dataset_left</span><span class="p">,</span> <span class="n">dataset_right</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_splitter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">feature_index</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
            
                <span class="c1"># check if partitions are not empty
</span>                <span class="nf">if </span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">dataset_left</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataset_right</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
                    <span class="c1"># Get target values of partitions
</span>                    <span class="n">Y_left</span><span class="p">,</span> <span class="n">Y_right</span> <span class="o">=</span> <span class="n">dataset_left</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dataset_right</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="c1"># Calculate variance reduction
</span>                    <span class="n">current_reduction</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">variance_reduction</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">Y_left</span><span class="p">,</span> <span class="n">Y_right</span><span class="p">)</span>

                    <span class="c1"># if the current reduction is better, update the best split 
</span>                    <span class="k">if</span> <span class="n">current_reduction</span> <span class="o">&gt;</span>  <span class="n">best_split</span><span class="p">[</span><span class="sh">"</span><span class="s">variance_reduction</span><span class="sh">"</span><span class="p">]</span> <span class="p">:</span>
                        
                        <span class="n">best_split</span><span class="p">[</span><span class="sh">"</span><span class="s">feature_index</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_index</span>
                        <span class="n">best_split</span><span class="p">[</span><span class="sh">"</span><span class="s">threshold</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">threshold</span>
                        <span class="n">best_split</span><span class="p">[</span><span class="sh">"</span><span class="s">dataset_left</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset_left</span>
                        <span class="n">best_split</span><span class="p">[</span><span class="sh">"</span><span class="s">dataset_right</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset_right</span>
                        <span class="n">best_split</span><span class="p">[</span><span class="sh">"</span><span class="s">variance_reduction</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_reduction</span>

        <span class="k">return</span> <span class="n">best_split</span>
    
    <span class="k">def</span> <span class="nf">_splitter</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">feature_index</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>

        <span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">left_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argwhere</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,</span> <span class="n">feature_index</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
        <span class="n">right_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argwhere</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,</span> <span class="n">feature_index</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
        
        <span class="n">dataset_left</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">left_indices</span><span class="p">]</span>
        <span class="n">dataset_right</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">right_indices</span><span class="p">]</span>
    
        <span class="k">return</span> <span class="n">dataset_left</span><span class="p">,</span> <span class="n">dataset_right</span>

    <span class="k">def</span> <span class="nf">variance_reduction</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Y_left</span><span class="p">,</span> <span class="n">Y_right</span><span class="p">):</span>

        <span class="c1"># Variance of parent and child left/right nodes
</span>        <span class="n">varY</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
        <span class="n">varY_left</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">Y_left</span><span class="p">)</span>
        <span class="n">varY_right</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">Y_right</span><span class="p">)</span>

        <span class="c1"># weights of child nodes
</span>        <span class="n">n</span><span class="p">,</span> <span class="n">nl</span><span class="p">,</span> <span class="n">nr</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">Y_left</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">Y_right</span><span class="p">)</span>
        <span class="n">w_left</span> <span class="o">=</span> <span class="n">nl</span> <span class="o">/</span> <span class="n">n</span>
        <span class="n">w_right</span> <span class="o">=</span> <span class="n">nr</span> <span class="o">/</span> <span class="n">n</span>

        <span class="k">return</span> <span class="n">varY</span> <span class="o">-</span> <span class="p">(</span><span class="n">w_left</span> <span class="o">*</span> <span class="n">varY_left</span> <span class="o">+</span> <span class="n">w_right</span> <span class="o">*</span> <span class="n">varY_right</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">compute_node</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="c1"># Get mean of target values
</span>        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="nf">_search_tree</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">root</span><span class="p">)</span> <span class="k">for</span> <span class="n">instance</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">Y_pred</span>

    <span class="k">def</span> <span class="nf">_search_tree</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">instance</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="c1"># Recursive function
</span>        <span class="k">if</span> <span class="n">node</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">node</span><span class="p">.</span><span class="n">leaf_value</span>
        
        <span class="c1"># check if instance value is less than node threshold
</span>        <span class="k">if</span> <span class="n">instance</span><span class="p">[</span><span class="n">node</span><span class="p">.</span><span class="n">feature_index</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">node</span><span class="p">.</span><span class="n">threshold</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_search_tree</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">left_node</span><span class="p">)</span>
        
        <span class="k">else</span><span class="p">:</span>  
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_search_tree</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">right_node</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">print_tree</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s"> function to print the tree </span><span class="sh">'''</span>

        <span class="k">if</span> <span class="n">node</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">node</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">root</span>

        <span class="c1"># Check for leaf nodes
</span>        <span class="k">if</span> <span class="n">node</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="sh">"</span><span class="s">Leaf Value:</span><span class="sh">"</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">leaf_value</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="sh">"</span><span class="s">Samples:</span><span class="sh">"</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">samples</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># Print information about this node: feature index, threshold, and variance reduction
</span>        <span class="nf">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="sh">"</span><span class="s">Feature X_</span><span class="sh">"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">feature_index</span><span class="p">))</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="sh">"</span><span class="s">Threshold:</span><span class="sh">"</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">threshold</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="sh">"</span><span class="s">Variance Reduction:</span><span class="sh">"</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">var_reduction</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="sh">"</span><span class="s">Samples:</span><span class="sh">"</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">samples</span><span class="p">)</span>

        <span class="c1"># Print the left subtree
</span>        <span class="nf">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="sh">"</span><span class="s">Left:</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">print_tree</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">left_node</span><span class="p">,</span> <span class="n">indent</span> <span class="o">+</span> <span class="sh">"</span><span class="s">  </span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Print the right subtree
</span>        <span class="nf">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="sh">"</span><span class="s">Right:</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">print_tree</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">right_node</span><span class="p">,</span> <span class="n">indent</span> <span class="o">+</span> <span class="sh">"</span><span class="s">  </span><span class="sh">"</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DecisionTreeVisualizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">decision_tree</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">feature_names</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decision_tree</span> <span class="o">=</span> <span class="n">decision_tree</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dot</span> <span class="o">=</span> <span class="nc">Digraph</span><span class="p">(</span><span class="n">comment</span><span class="o">=</span><span class="sh">'</span><span class="s">Decision Tree</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">node_counter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">add_nodes_edges</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">parent_id</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">""</span><span class="p">):</span>
        <span class="n">node_id</span> <span class="o">=</span> <span class="nf">str</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">node_counter</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">node_counter</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="n">samples</span> <span class="o">=</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="sh">"</span><span class="s">samples</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>        
        <span class="c1"># Display relevant information in the nodes
</span>        <span class="k">if</span> <span class="n">node</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="n">display_label</span> <span class="o">=</span> <span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Leaf</span><span class="se">\n</span><span class="sh">'</span>
                             <span class="sa">f</span><span class="sh">'</span><span class="s">Samples: </span><span class="si">{</span><span class="n">samples</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span>
                             <span class="sa">f</span><span class="sh">'</span><span class="s">Value: </span><span class="si">{</span><span class="n">node</span><span class="p">.</span><span class="n">leaf_value</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">display_label</span> <span class="o">=</span> <span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="n">node</span><span class="p">.</span><span class="n">feature_index</span><span class="p">]</span><span class="si">}</span><span class="s"> ≤ </span><span class="si">{</span><span class="n">node</span><span class="p">.</span><span class="n">threshold</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span>
                             <span class="sa">f</span><span class="sh">'</span><span class="s">Samples: </span><span class="si">{</span><span class="n">samples</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span> 
                             <span class="sa">f</span><span class="sh">'</span><span class="s">Value: </span><span class="si">{</span><span class="n">node</span><span class="p">.</span><span class="n">node_value</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">dot</span><span class="p">.</span><span class="nf">node</span><span class="p">(</span><span class="n">node_id</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">display_label</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">parent_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">dot</span><span class="p">.</span><span class="nf">edge</span><span class="p">(</span><span class="n">parent_id</span><span class="p">,</span> <span class="n">node_id</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">node</span><span class="p">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">add_nodes_edges</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">left_node</span><span class="p">,</span> <span class="n">node_id</span><span class="p">,</span> <span class="sh">'</span><span class="s">Yes</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">add_nodes_edges</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">right_node</span><span class="p">,</span> <span class="n">node_id</span><span class="p">,</span> <span class="sh">'</span><span class="s">No</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">visualize_tree</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">add_nodes_edges</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">decision_tree</span><span class="p">.</span><span class="n">root</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">dot</span>
</code></pre></div></div>

<p><strong>Example 1</strong></p>

<p>Let’s train a decision tree regressor using the scikit-learn library and the algorithm that we created in python to predict the <code class="language-plaintext highlighter-rouge">median_house_value</code> variable to compare both results. Let’s train the models with <code class="language-plaintext highlighter-rouge">max_depth=1</code>. Which feature is used for splitting the data? The feature used to split the data is the same in both models?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># My Model
</span><span class="n">mytree</span> <span class="o">=</span> <span class="nc">MyDecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">mytree</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">mytree</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">feature_names</span><span class="o">=</span><span class="n">dv</span><span class="p">.</span><span class="n">feature_names_</span>
<span class="n">visualizer</span> <span class="o">=</span> <span class="nc">DecisionTreeVisualizer</span><span class="p">(</span><span class="n">mytree</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span> <span class="n">dv</span><span class="p">.</span><span class="n">feature_names_</span><span class="p">)</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">visualizer</span><span class="p">.</span><span class="nf">visualize_tree</span><span class="p">()</span>
<span class="nf">display</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">rmse</span><span class="sh">'</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>

<span class="c1"># Running Time : 5.8 s
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rmse 0.454424874819611
</code></pre></div></div>

<center>
<img src="/assets/img/post3/my-decision-tree.png" width="500" height="350" />
</center>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># scikit-learn Model
</span><span class="n">tree</span> <span class="o">=</span> <span class="nc">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tree</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">feature_names</span><span class="o">=</span><span class="n">dv</span><span class="p">.</span><span class="n">feature_names_</span>
<span class="nf">plot_tree</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span> <span class="n">dv</span><span class="p">.</span><span class="n">feature_names_</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">rmse</span><span class="sh">'</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>

<span class="c1"># Running Time : 0.1 s
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rmse 0.4544248748196092
</code></pre></div></div>

<center>
<img src="/assets/img/post3/decision-tree-scikit.png" width="500" height="400" />
</center>

<h1 id="algorithm-for-random-forest"><strong>Algorithm for Random Forest</strong></h1>

<p>A central characteristic of decision tree models is their instability. A minor perturbation in the training dataset can significantly affect the construction of the tree, thereby altering its predictions or decision boundaries. This kind of instability is often observed in systems sensitive to initial conditions, a concept that we can make a parallel in chaos theory. In chaos theory, those systems are central and show dynamic behaviors that are highly dependent on their starting conditions, often referred to as the “butterfly effect”</p>

<p>This sensitivity is clearly manifested in decision trees. The decisions made at the root node will influence the overall tree structure. A single anomalous instance, like an instance with NaN values or zeros, can drastically influence these initial decisions, leading to a different pattern and predictions. To mitigate this issue, the Random Forest algorithm was introduced.</p>

<p>The Random Forest algorithm is an ensemble learning method utilized for both classification and regression tasks. A random forest is simply an ensemble of $k$ decision trees, where each tree is constructed from a slightly perturbed dataset. These datasets are created through bootstrapping, a technique that resamples the original dataset with replacement. Furthermore, each tree is built by sampling a random subset of features at each internal node during the decision tree’s construction.</p>

<p>The random sampling of features aims to introduce diversity among the trees,  therefore reducing their correlation and improving the ensemble’s generalization. By averaging the predictions from the individual trees, the ensemble model more effectively captures the variations in the data, resulting in a more stable and accurate predictive model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MyRandomForestRegressor</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>  <span class="n">n_estimators</span><span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_split</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                  <span class="n">max_depth</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n_estimators</span>
        <span class="n">self</span><span class="p">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="n">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="n">self</span><span class="p">.</span><span class="n">trees</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_estimators</span><span class="p">):</span>
            
            <span class="n">bootstrap_X</span><span class="p">,</span> <span class="n">bootstrap_Y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_bootstrap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
            <span class="n">tree</span> <span class="o">=</span> <span class="nc">MyDecisionTreeRegressor</span><span class="p">(</span><span class="n">min_samples_split</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">min_samples_split</span><span class="p">,</span>
                                           <span class="n">max_depth</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">max_depth</span><span class="p">)</span>
            <span class="n">tree</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">bootstrap_X</span><span class="p">,</span> <span class="n">bootstrap_Y</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">trees</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_bootstrap</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Create a bootstrap sample
</span>        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">tree</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">trees</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Example 2</strong></p>

<p>Now let’s compare both models for the random forest using the following parameters:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n_estimators=10</code></li>
  <li><code class="language-plaintext highlighter-rouge">random_state=1</code></li>
  <li><code class="language-plaintext highlighter-rouge">n_jobs=-1</code> (to make training faster)</li>
</ul>

<p>What’s the RMSE on validation? How close are the result of both models?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># My Model
</span><span class="n">rf</span> <span class="o">=</span> <span class="nc">MyRandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">rf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">rf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">rmse = </span><span class="sh">'</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>
<span class="c1"># Running Time :6m 25s
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rmse =  0.24526091708215395
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># scikit-learn Model
</span><span class="n">rf</span> <span class="o">=</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">rf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">rmse = </span><span class="sh">'</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>
<span class="c1"># Running Time : 0.1s
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rmse =  0.244910835217013
</code></pre></div></div>

<h1 id="references">References</h1>

<ul>
  <li><a href="https://books.google.com/books/about/Data_Mining_and_Machine_Learning.html?id=oafDDwAAQBAJ&amp;printsec=frontcover&amp;source=kp_read_button&amp;hl=en&amp;newbks=1&amp;newbks_redir=1">Zaki, M. J., &amp; Meira Jr, W. (2020). Data mining and machine learning: fundamental concepts and algorithms. Cambridge University Press.</a></li>
</ul>


  
    
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-marcosbenicio-github-io-1.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



</div>


    </div>

  </body>
</html>

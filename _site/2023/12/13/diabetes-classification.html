<!DOCTYPE html>




<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <meta property="og:image" content="/assets/img/post4/bmi_distribution.png"/>
    
    
    <link rel="stylesheet" type="text/css" href="/assets/css/syntax_highlight.css">
    <link rel="stylesheet" type="text/css" href="/assets/css/main.css">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed" href="atom.xml" />

    <!-- MathJax Configuration -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true
          }
        });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

    <script>
      function show_tag_section(tag) {
        // Hide all other tag divs
        document.getElementById('all_posts').style.display = 'none';
        var tag_divs = document.getElementsByClassName('by_tag');
        var i;
        for (var i = 0; i < tag_divs.length; i++) {
          tag_divs[i].style.display = 'none';
        }
        // Show the one we want
        document.getElementById(tag).style.display = 'block';
      }
    </script>
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Analysis of Diabetes Indicators | Marcos Benício</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Analysis of Diabetes Indicators" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Diabetes is among the most prevalent chronic diseases in the United States, impacting millions of Americans each year. Diabetes is a chronic disease in which individuals lose the ability to effectively regulate levels of glucose in the blood, and can lead to reduced quality of life and life expectancy. This project uses the Diabetes Health Indicators dataset, available on kaggle. The project seeks answer the following questions: Which risk factors most strongly predict diabetes? Can a subset of the risk factors to accurately predict whether an individual has diabetes?" />
<meta property="og:description" content="Diabetes is among the most prevalent chronic diseases in the United States, impacting millions of Americans each year. Diabetes is a chronic disease in which individuals lose the ability to effectively regulate levels of glucose in the blood, and can lead to reduced quality of life and life expectancy. This project uses the Diabetes Health Indicators dataset, available on kaggle. The project seeks answer the following questions: Which risk factors most strongly predict diabetes? Can a subset of the risk factors to accurately predict whether an individual has diabetes?" />
<link rel="canonical" href="http://localhost:4000/2023/12/13/diabetes-classification.html" />
<meta property="og:url" content="http://localhost:4000/2023/12/13/diabetes-classification.html" />
<meta property="og:site_name" content="Marcos Benício" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-13T00:00:00-03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Analysis of Diabetes Indicators" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-12-13T00:00:00-03:00","datePublished":"2023-12-13T00:00:00-03:00","description":"Diabetes is among the most prevalent chronic diseases in the United States, impacting millions of Americans each year. Diabetes is a chronic disease in which individuals lose the ability to effectively regulate levels of glucose in the blood, and can lead to reduced quality of life and life expectancy. This project uses the Diabetes Health Indicators dataset, available on kaggle. The project seeks answer the following questions: Which risk factors most strongly predict diabetes? Can a subset of the risk factors to accurately predict whether an individual has diabetes?","headline":"Analysis of Diabetes Indicators","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/12/13/diabetes-classification.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/profile_photo.png"}},"url":"http://localhost:4000/2023/12/13/diabetes-classification.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body onload="show_tag_section('all_posts')">

    <div class="header">

      <!-- Large header banner on left for large display widths -->
      <div class="big_header">

        <!-- Name -->
        <div class="name_group">
          <h1><a href="/">Marcos Benício</a></h1>
        </div>

        <!-- <a href=""><img src="/assets/img/profile_photo.png" alt="Logo" class="center_img" ></a>-->
        <a href="/"><img src="/assets/img/profile_photo.png" alt="Logo" class="center_img" ></a>
        <!--<a href=""><img src="/assets/img/profile_photo.png" alt="Logo" class="center_img" ></a>-->



        <!-- Position + company -->
        <div class="link_group">

          <div class="row">
              <div class="col1">
                <span> M.sc in Physics</span>
              </div>
          </div>

          <div class="row">
            <!-- <a href=""-->
              <div class="col1">
                <img src="/assets/img/brazil_icon.png" height="30" width="30">
                <!-- <span>  </span>-->
              </div>
            </a>
          </div>

          <div class="row">
            <div class="col1">
              <img src="/assets/img/place_icon_light.png" height="16" width="16">
              <span> Niterói, RJ </span>
            </div>
          </div>

        </div>

        <!-- Badges/links -->
        <div class="link_group">

          <div class="row">
            <a href="https://github.com/marcosbenicio">
              <div class="col2">
                <img src="/assets/img/github_icon_light.png" height="16" width="16">
                <span> Github </span>
              </div>
            </a>
            <a href="https://linkedin.com/in/marcos-benício-de-andrade-alonso-415a5b16b">
              <div class="col2">
                <img src="/assets/img/linkedin_icon_light.png" height="16" width="16">
                <span> LinkedIn </span>
              </div>
            </a>
          </div>

          <div class="row">
            <a href="mailto:marcosbenicio0102@gmail.com">
              <div class="col2">
                <img src="/assets/img/email_icon_light.png" height="16" width="16">
                <span> Email </span>
              </div>
            </a>
            <a href="/assets/files/resume.pdf">
              <div class="col2">
                <img src="/assets/img/cv_icon_light.png" height="16" width="16">
                <span> Resume </span>
              </div>
            </a>
          </div>

        </div>

      </div>

      <!-- Smaller header banner on top for small display widths -->
      <div class="small_header">

        <!-- Name -->
        <div class="small_header_box">
          <div class="name_header_box">
            <div class="name_header_img">
              <a href="/">
                <img src="/assets/img/profile_photo.png" alt="Logo" height="60">
              </a>
            </div>
            <div class="name_header_name">
              <h1><a href="/">Marcos Benício</a></h1>
            </div>
          </div>
        </div>

        <!-- Position + Company -->
        <div class="small_header_box">
          <div class="row">
            <div class="col1">
              <span> M.sc in Physics </span>
            </div>
          </div>
          <div class="row">
            <div class="col1">
             <!-- <a href=""> -->
                  <img src="/assets/img/brazil_icon.png" height="30" width="30">
                  <!-- <span>  </span> -->
              </a>
            </div>
          </div>
        </div>

        <!-- Badges/links -->
        <div class="small_header_box">
                <div class="row">
                  <a href="https://github.com/marcosbenicio">
                    <div class="col2">
                      <img src="/assets/img/github_icon_light.png" height="16" width="16">
                      <span> Github </span>
                    </div>
                  </a>
                  <a href="https://linkedin.com/in/marcos-benício-de-andrade-alonso-415a5b16b">
                    <div class="col2">
                      <img src="/assets/img/linkedin_icon_light.png" height="16" width="16">
                      <span> LinkedIn </span>
                    </div>
                  </a>
                </div>

                <div class="row">
                  <a href="mailto:marcosbenicio0102@gmail.com">
                    <div class="col2">
                      <img src="/assets/img/email_icon_light.png" height="16" width="16">
                      <span> Email </span>
                    </div>
                  </a>
                  <a href="/assets/files/resume.pdf">
                    <div class="col2">
                      <img src="/assets/img/cv_icon_light.png" height="16" width="16">
                      <span> Resume </span>
                    </div>
                  </a>
                </div>
        </div>

      </div>

    </div>

    <!-- Page content -->
    <div class="content">

      <h1>Analysis of Diabetes Indicators</h1>
<p class="meta">13 Dec 2023 - Tags: Classification, Random Forest, Decision Tree, and Logistic Regression</p>

<div class="button_container">
  
    <a href="https://github.com/marcosbenicio/diabetes-classification">
      <div class="button_link">
        View on<br /><strong>Github</strong>
      </div>
    </a>
  
  
  
</div>


<div class="post">
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">auc</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span><span class="n">recall_score</span> <span class="p">,</span><span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span>\
                            <span class="n">f1_score</span><span class="p">,</span> <span class="n">roc_curve</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span>  <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">plot_tree</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="n">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">mutual_info_classif</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="n">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2_contingency</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="n">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="n">catboost</span> <span class="kn">import</span> <span class="n">CatBoostClassifier</span><span class="p">,</span> <span class="n">Pool</span>

<span class="c1"># Palette of colors used for the plots
</span><span class="n">red</span>  <span class="o">=</span> <span class="sh">'</span><span class="s">#BE3232</span><span class="sh">'</span>
<span class="n">blue</span> <span class="o">=</span> <span class="sh">'</span><span class="s">#2D4471</span><span class="sh">'</span> 
</code></pre></div></div>

<h1 id="outline"><strong>Outline</strong></h1>

<ul>
  <li><a href="#1-data-preprocessing"><strong>1. Data preparation</strong></a>
    <ul>
      <li><a href="#11-split-dataset"><strong>1.1. Split Dataset</strong></a></li>
    </ul>
  </li>
  <li><a href="#2-exploratory-data-analysis-eda"><strong>2. Exploratory Data Analysis (EDA)</strong></a>
    <ul>
      <li><a href="#21-feature-distribution"><strong>2.1 Feature Distribution</strong></a></li>
      <li><a href="#22-feature-importance"><strong>2.2 Feature Importance</strong></a></li>
      <li><a href="#23-conclusion-from-eda"><strong>2.3 Conclusion from EDA</strong></a></li>
    </ul>
  </li>
  <li><a href="#3-model-training-and-validation"><strong>3. Model Training and Validation</strong></a>
    <ul>
      <li><a href="#31-logistic-regression"><strong>3.1 Logistic Regression</strong></a>
        <ul>
          <li><a href="#311-hyperparameter-tunning-and-validate"><strong>3.1.1 Hyperparameter tunning and validate</strong></a></li>
          <li><a href="#312-feature-importance"><strong>3.1.2 Feature importance</strong></a></li>
          <li><a href="#313-metrics"><strong>3.1.3 Metrics</strong></a></li>
        </ul>
      </li>
      <li><a href="#32-decision-tree-and-random-forest"><strong>3.2 Decision Tree and Random Forest</strong></a>
        <ul>
          <li><a href="#321-hyperparameter-tunning-and-validate"><strong>3.2.1 Hyperparameter Tunning And Validate</strong></a></li>
          <li><a href="#322-feature-importance"><strong>3.2.2 Feature Importance</strong></a></li>
          <li><a href="#323-metrics"><strong>3.2.3 Metrics</strong></a></li>
        </ul>
      </li>
      <li><a href="#33-selecting-model-and-best-set-of-features"><strong>3.3 Selecting Model And Best Set Of Features</strong></a></li>
    </ul>
  </li>
  <li><a href="#4-model-evaluation-on-test-set"><strong>4. Model Evaluation on Test Set</strong></a></li>
  <li><a href="#5-conclusions"><strong>5. Conclusions</strong></a></li>
</ul>

<h1 id="1-data-preprocessing"><strong>1. Data Preprocessing</strong></h1>

<p>The dataset used here is from <a href="https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_012_health_indicators_BRFSS2015.csv">kaggle</a>. Let’s check the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">../data/raw/diabete-indicators-balanced-5050split.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">display</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Before:</span><span class="sh">"</span><span class="p">,</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># Search for duplicated instances and drop
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Duplications:</span><span class="sh">"</span><span class="p">,</span><span class="n">df</span><span class="p">.</span><span class="nf">duplicated</span><span class="p">().</span><span class="nf">sum</span><span class="p">())</span>
<span class="c1">#display(df.loc[df.duplicated(),:].head())
</span><span class="n">df</span><span class="p">.</span><span class="nf">drop_duplicates</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">After:</span><span class="sh">'</span><span class="p">,</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Diabetes_binary</th>
      <th>HighBP</th>
      <th>HighChol</th>
      <th>CholCheck</th>
      <th>BMI</th>
      <th>Smoker</th>
      <th>Stroke</th>
      <th>HeartDiseaseorAttack</th>
      <th>PhysActivity</th>
      <th>Fruits</th>
      <th>...</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>26.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>26.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>...</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 22 columns</p>
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Before: (70692, 22)
Duplications: 1635
After: (69057, 22)
</code></pre></div></div>

<p>Let’s check if there are zeros in the categorical features where the range is above zero. This can be interpreted as missing values, so we need to change then to NaN.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">categorical_columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">GenHlth</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">MentHlth</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">PhysHlth</span><span class="sh">"</span><span class="p">,</span>
               <span class="sh">"</span><span class="s">Age</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Income</span><span class="sh">"</span><span class="p">]</span>
<span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">categorical_columns</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Missing Values:</span><span class="sh">'</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nf">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span><span class="o">/</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s"> %</span><span class="sh">'</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">in </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Missing Values: </span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">BMI</span><span class="sh">'</span><span class="p">].</span><span class="nf">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span><span class="si">}</span><span class="s"> % in BMI</span><span class="sh">"</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">MentHlth</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">PhysHlth</span><span class="sh">'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">categorical_columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">GenHlth</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">MentHlth</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">PhysHlth</span><span class="sh">"</span><span class="p">,</span>
               <span class="sh">"</span><span class="s">Age</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Income</span><span class="sh">"</span><span class="p">]</span>
<span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">categorical_columns</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Missing Values:</span><span class="sh">'</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nf">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span><span class="o">/</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s"> %</span><span class="sh">'</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">in </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Missing Values: </span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">BMI</span><span class="sh">'</span><span class="p">].</span><span class="nf">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span><span class="si">}</span><span class="s"> % in BMI</span><span class="sh">"</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">MentHlth</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">PhysHlth</span><span class="sh">'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>There are indeed missing values for <code class="language-plaintext highlighter-rouge">MentHlth</code> and <code class="language-plaintext highlighter-rouge">PhysHlth</code>, both must have a range between 1-30. But because the missing values account for 67% of my dataset, it’s concerning. Even if this feature correlate  highly with the target, removing 67% of data might be worse. A straightforward solution was just to drop these features.</p>

<h2 id="11-split-dataset"><strong>1.1. Split Dataset</strong></h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">convert_dtypes</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">dtype_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Convert the data types of specified columns for memory optimization.

    Parameters:
        df (pd.DataFrame): The original DataFrame.
        dtype_dict (dict): Dictionary specifying the columns and their corresponding data types.
                           Key: Desired data type (e.g., </span><span class="sh">'</span><span class="s">bool</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">int32</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="s">)
                           Value: List of columns to be converted to the key data type

    Returns:
        pd.DataFrame: DataFrame with optimized data types.
    </span><span class="sh">"""</span>
    <span class="k">for</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">columns</span> <span class="ow">in</span> <span class="n">dtype_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
            <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            

    <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># Before split, change the target name to Diabetes
</span><span class="n">df</span><span class="p">.</span><span class="nf">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">Diabetes_binary</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Split in 60% Train /20% Validation/ 20% Test
</span><span class="n">df_train_large</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">df_train</span><span class="p">,</span> <span class="n">df_val</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">df_train_large</span><span class="p">,</span> <span class="n">train_size</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>


<span class="c1"># Save target feature
</span><span class="n">Y_train_large</span> <span class="o">=</span> <span class="n">df_train_large</span><span class="p">[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>
<span class="n">Y_val</span> <span class="o">=</span> <span class="n">df_val</span><span class="p">[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>

<span class="c1"># Drop target feature 
</span><span class="n">df_train</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_val</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_test</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>We will adjust the data types of <code class="language-plaintext highlighter-rouge">df_train_large</code> to optimize memory usage during the EDA process.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># for better memory usage and interpretability for EDA
</span><span class="n">boolean_columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">HighBP</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">HighChol</span><span class="sh">"</span><span class="p">,</span> 
               <span class="sh">"</span><span class="s">CholCheck</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Smoker</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Stroke</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">HeartDiseaseorAttack</span><span class="sh">"</span><span class="p">,</span>
               <span class="sh">"</span><span class="s">PhysActivity</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Fruits</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Veggies</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">HvyAlcoholConsump</span><span class="sh">"</span><span class="p">,</span>  
               <span class="sh">"</span><span class="s">AnyHealthcare</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">NoDocbcCost</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">DiffWalk</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Sex</span><span class="sh">"</span><span class="p">]</span>

<span class="n">categorical_columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">GenHlth</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Age</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Income</span><span class="sh">"</span><span class="p">]</span>

<span class="n">dtype_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">int32</span><span class="sh">'</span><span class="p">:</span> <span class="n">categorical_columns</span> <span class="o">+</span> <span class="n">boolean_columns</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">BMI</span><span class="sh">"</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">df_train_large</span> <span class="o">=</span> <span class="nf">convert_dtypes</span><span class="p">(</span><span class="n">df_train_large</span><span class="p">,</span> <span class="n">dtype_dict</span><span class="p">)</span>
<span class="nf">display</span><span class="p">(</span><span class="n">df_train_large</span><span class="p">.</span><span class="nf">info</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 55245 entries, 15055 to 5239
Data columns (total 20 columns):
 #   Column                Non-Null Count  Dtype  
---  ------                --------------  -----  
 0   Diabetes              55245 non-null  int32  
 1   HighBP                55245 non-null  int32  
 2   HighChol              55245 non-null  int32  
 3   CholCheck             55245 non-null  int32  
 4   BMI                   55245 non-null  float32
 5   Smoker                55245 non-null  int32  
 6   Stroke                55245 non-null  int32  
 7   HeartDiseaseorAttack  55245 non-null  int32  
 8   PhysActivity          55245 non-null  int32  
 9   Fruits                55245 non-null  int32  
 10  Veggies               55245 non-null  int32  
 11  HvyAlcoholConsump     55245 non-null  int32  
 12  AnyHealthcare         55245 non-null  int32  
 13  NoDocbcCost           55245 non-null  int32  
 14  GenHlth               55245 non-null  int32  
 15  DiffWalk              55245 non-null  int32  
 16  Sex                   55245 non-null  int32  
 17  Age                   55245 non-null  int32  
 18  Education             55245 non-null  int32  
 19  Income                55245 non-null  int32  
dtypes: float32(1), int32(19)
memory usage: 4.6 MB
</code></pre></div></div>

<h1 id="2-exploratory-data-analysis-eda"><strong>2. Exploratory Data Analysis (EDA)</strong></h1>

<p>Let’s proceed with the EDA in the <code class="language-plaintext highlighter-rouge">df_train_large</code> dataset, which is the combination of both training and validation sets. To prevent data leakage, we don’t include the test set for the EDA. This will ensure  the trained model evaluation on the test set is unbiased, without information from the train set mistakenly leakage to the test.</p>

<h2 id="21-feature-distribution"><strong>2.1 Feature Distribution</strong></h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a dataset without the target feature
</span><span class="n">df_train_dropped</span> <span class="o">=</span> <span class="n">df_train_large</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="n">df_train_dropped</span><span class="p">.</span><span class="n">columns</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="c1"># Identify row and column in the grid
</span>    <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="nf">divmod</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    
    <span class="c1"># Check if the feature is boolean type
</span>    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nf">issubdtype</span><span class="p">(</span><span class="n">df_train_dropped</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">bool_</span><span class="p">):</span>
        <span class="n">Yes_count</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">df_train_large</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>
        <span class="n">No_count</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">df_train_large</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span> <span class="o">-</span> <span class="n">Yes_count</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">].</span><span class="nf">bar</span><span class="p">([</span><span class="sh">'</span><span class="s">Yes</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">No</span><span class="sh">'</span><span class="p">],</span> <span class="p">[</span><span class="n">Yes_count</span><span class="p">,</span> <span class="n">No_count</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Plot histogram for the current feature
</span>        <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">].</span><span class="nf">hist</span><span class="p">(</span><span class="n">df_train_dropped</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,)</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">].</span><span class="nf">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># remove empty plots
</span><span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()[</span><span class="nf">len</span><span class="p">(</span><span class="n">features</span><span class="p">):]:</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Adjust layout
</span><span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/feature_distribution.png" width="800" height="900" />
</center>

<p>Most of our features are categorical. Features like <strong>CholCHeck</strong>, <strong>Stroke</strong>, <strong>HeartDiseaseorAttack</strong>, <strong>Vaggies</strong>, <strong>HvyAlcoholConsump</strong>,  <strong>AnyHealthcare</strong>, <strong>NoDocbcCost</strong> are highly concentrated in one category. This suggests that there may be a common characteristic shared among most individuals for each of these features, resulting in skewed distributions. While this skewness in feature distribution does not present the same challenges as an imbalanced target, it may still impact the interpretability and performance of our predictive models, particularly if these features are strongly associated with the target.</p>

<p>Also, it’s appear that the feature <strong>BMI</strong> has outliers that wee need to take a closer look.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From World Health Organization (WHO):
#---------------------------
# Under: BMI less than 18.5
# Normal: BMI between 18.5 and 24.9
# Over: BMI between 25 and 29.9
# Obesity: BMI of 30 or greater
</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Plot the histplot on the first axes
</span><span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">df_train_dropped</span><span class="p">[</span><span class="sh">'</span><span class="s">BMI</span><span class="sh">'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Add vertical lines
</span><span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">18.5</span><span class="p">,</span> <span class="mf">24.9</span><span class="p">,</span> <span class="mf">29.9</span><span class="p">]:</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axvline</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span> <span class="n">red</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Add text labels within the second subplot (histplot)
</span><span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Under</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Normal</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Over</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Obesity</span><span class="sh">'</span><span class="p">]</span>
<span class="n">positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mf">21.7</span><span class="p">,</span> <span class="mf">27.45</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>  <span class="c1"># Midpoints between the lines and the ends
</span>
<span class="k">for</span> <span class="n">x_pos</span><span class="p">,</span> <span class="n">cat</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="n">categories</span><span class="p">):</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">text</span><span class="p">(</span><span class="n">x_pos</span><span class="p">,</span> <span class="mi">5500</span><span class="p">,</span> <span class="n">cat</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="sh">'</span><span class="s">bottom</span><span class="sh">'</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">BMI Histogram</span><span class="sh">'</span><span class="p">)</span>

<span class="c1">#-------
# Plot the boxplot on the second axes
</span><span class="n">sns</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_train_dropped</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">BMI</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">fliersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">BMI Boxplot</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Add horizontal lines
</span><span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">18.5</span><span class="p">,</span> <span class="mf">24.9</span><span class="p">,</span> <span class="mf">29.9</span><span class="p">]:</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axhline</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span> <span class="n">red</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>


<span class="c1"># Display the plots
</span><span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/outliers_distribution.png" width="800" height="500" />
</center>

<p>There are numerous outliers, indicating individuals with significantly higher BMI values. Given the skewed distribution and box plot, these outliers do not necessarily represent errors or anomalies in the data but rather a realistic representation of the upper bound of BMI within the population.</p>

<h2 id="22-feature-importance"><strong>2.2 Feature Importance</strong></h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_feature_importance</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.002</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Feature Importance</span><span class="sh">'</span><span class="p">,</span> 
                            <span class="n">xlabel</span><span class="o">=</span><span class="sh">'</span><span class="s">Features</span><span class="sh">'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sh">'</span><span class="s">Importance</span><span class="sh">'</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Function to plot the feature importance with a distinction of importance based on a threshold.

    Parameters:
    - df: pandas.DataFrame
        DataFrame containing features and their importance scores.
    - x: str
        Name of the column representing feature names.
    - y: str
        Name of the column representing feature importance scores.
    - ax: matplotlib axis object
        Axis on which to draw the plot.
    - threshold: float, optional (default=0.002)
        Value above which bars will be colored differently.
    - pad: float, optional (default=5.0)
        Adjust the layout of the plot.
    - title: str, optional (default=</span><span class="sh">'</span><span class="s">Feature Importance</span><span class="sh">'</span><span class="s">)
        Title of the plot.
    - xlabel: str, optional (default=</span><span class="sh">'</span><span class="s">Features</span><span class="sh">'</span><span class="s">)
        Label for the x-axis.
    - ylabel: str, optional (default=</span><span class="sh">'</span><span class="s">Importance</span><span class="sh">'</span><span class="s">)
        Label for the y-axis.
    - palette: list, optional
        A list of two colors. The first color is for bars below the threshold and the second
        is for bars above.

    Returns:
    - None (modifies ax in-place)
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">palette</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">palette</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">]</span>
    
    <span class="n">blue</span><span class="p">,</span> <span class="n">red</span> <span class="o">=</span> <span class="n">palette</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">red</span> <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;=</span> <span class="n">threshold</span> <span class="k">else</span> <span class="n">blue</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="n">y</span><span class="p">]]</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xticklabels</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span> 
    <span class="n">ax</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Mutual Information</strong></p>

<p>Given that most of our features are binary, having only one numerical feature  for <code class="language-plaintext highlighter-rouge">BMI</code>, a first natural choice of metric to account correlation would be the Mutual information (MI). MI accounts for linear and non-linear relation, is robust against outliers and can be reliable for any numerical or categorical features.</p>

<p>Let’s compare the MI of the features with the target variable <code class="language-plaintext highlighter-rouge">Diabetes</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mi_values</span> <span class="o">=</span> <span class="nf">mutual_info_classif</span><span class="p">(</span><span class="n">df_train_dropped</span><span class="p">,</span> <span class="n">Y_train_large</span><span class="p">,</span> <span class="n">discrete_features</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">mi_column</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">mi_values</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nc">Index</span><span class="p">(</span><span class="n">df_train_dropped</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">),</span> 
                         <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">diabetes_MI</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="sh">'</span><span class="s">diabetes_MI</span><span class="sh">'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Convert the index 'Features' into a column for plotting
</span><span class="n">mi_column_reset</span> <span class="o">=</span> <span class="n">mi_column</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">()</span>

<span class="c1"># Plotting
</span><span class="n">feature_threshold</span><span class="o">=</span> <span class="mf">0.020</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="nf">plot_feature_importance</span><span class="p">(</span><span class="n">mi_column_reset</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">diabetes_MI</span><span class="sh">'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">feature_threshold</span><span class="p">,</span> 
                        <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Mutual Information With Diabetes</span><span class="sh">"</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Features</span><span class="sh">"</span><span class="p">,</span> 
                        <span class="n">ylabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Diabetes MI</span><span class="sh">"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="p">[</span><span class="n">blue</span><span class="p">,</span> <span class="n">red</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Select features with MI &gt; feature_threshold
</span><span class="n">best_mi_features</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">mi_column</span><span class="p">[</span><span class="n">mi_column</span><span class="p">[</span><span class="sh">'</span><span class="s">diabetes_MI</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">feature_threshold</span><span class="p">].</span><span class="n">index</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Best correlated features:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">best_mi_features</span><span class="p">)</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/mi_diabetes.png" width="800" height="500" />
</center>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Best correlated features:
 ['GenHlth', 'HighBP', 'BMI', 'Age', 'HighChol', 'DiffWalk', 'Income', 'HeartDiseaseorAttack']
</code></pre></div></div>

<p>The bars in red represent the features related to the target variable <strong>Diabetes</strong> that preset the most significant entropy reduction. This means that these features provide significant information about the presence of diabetes.</p>

<p><strong>Spearman Correlation</strong></p>

<p>Some of our features, like <strong>GenHlth</strong>, <strong>MentHlth</strong>, <strong>PhysHlth</strong>, `<strong>Age</strong>, <strong>Education</strong>, <strong>Income</strong> are ordinal. A useful metric for this case is Spearman Correlation. Spearman Correlation is a non-parametric metric (does not assume any specific distribution for the data) and are more reliable to ordinal data. It captures monotonic relationships, whether linear or not.</p>

<p>Let’s compare the Spearman Correlation of the features with the target variable <code class="language-plaintext highlighter-rouge">Diabetes</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate Spearman correlation
</span><span class="n">spearman_corr</span> <span class="o">=</span> <span class="n">df_train_large</span><span class="p">.</span><span class="nf">corr</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">spearman</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">].</span><span class="nf">agg</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">)</span>\
                                                <span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>\
                                                <span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">)</span>\
                                                <span class="p">.</span><span class="nf">to_frame</span><span class="p">()</span>

<span class="c1"># Convert Spearman correlation to DataFrame and reset index
</span><span class="n">spearman_column</span> <span class="o">=</span> <span class="n">spearman_corr</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">()</span>
<span class="n">spearman_column</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Features</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">spearman_correlation</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># Plotting
</span><span class="n">feature_threshold</span><span class="o">=</span> <span class="mf">0.07</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="nf">plot_feature_importance</span><span class="p">(</span><span class="n">spearman_column</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">Features</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">spearman_correlation</span><span class="sh">'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> 
                        <span class="n">threshold</span><span class="o">=</span><span class="n">feature_threshold</span><span class="p">,</span> 
                        <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Spearman Correlation With Diabetes</span><span class="sh">"</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Features</span><span class="sh">"</span><span class="p">,</span> 
                        <span class="n">ylabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Diabetes Correlation</span><span class="sh">"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="p">[</span><span class="n">blue</span><span class="p">,</span> <span class="n">red</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Select features with Spearman correlation value &gt; feature_threshold
</span><span class="n">best_spearman_features</span> <span class="o">=</span> <span class="n">spearman_corr</span><span class="p">[</span><span class="n">spearman_corr</span><span class="p">[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">feature_threshold</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Best correlated features:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">best_spearman_features</span><span class="p">)</span>
</code></pre></div></div>
<center>
<img src="/assets/img/post4/spearman_diabetes.png" width="800" height="500" />
</center>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Best correlated features:
 ['GenHlth', 'HighBP', 'BMI', 'HighChol', 'DiffWalk', 'Age', 'Income', 'HeartDiseaseorAttack', 
 'Education', 'PhysActivity', 'Stroke', 'CholCheck', 'HvyAlcoholConsump', 'Smoker', 'Veggies']
</code></pre></div></div>

<p>Let’s check one more metric, Pearson correlation. While Spearman detects monotonic relationships, Pearson captures linear relationships.</p>

<p><strong>Pearson Correlation</strong></p>

<p>The Pearson correlation accounts only for linear relationships and is very sensitive to outliers. It’s not particularly reliable for categorical data, especially binary ones.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate Spearman correlation
</span><span class="n">pearson_corr</span> <span class="o">=</span> <span class="n">df_train_large</span><span class="p">.</span><span class="nf">corr</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">pearson</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">].</span><span class="nf">agg</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">)</span>\
                                                <span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>\
                                                <span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">)</span>\
                                                <span class="p">.</span><span class="nf">to_frame</span><span class="p">()</span>

<span class="c1"># Convert Spearman correlation to DataFrame and reset index
</span><span class="n">pearson_column</span> <span class="o">=</span> <span class="n">pearson_corr</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">()</span>
<span class="n">pearson_column</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Features</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">pearson_correlation</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># 
</span><span class="n">feature_threshold</span> <span class="o">=</span> <span class="mf">0.07</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="nf">plot_feature_importance</span><span class="p">(</span><span class="n">pearson_column</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">Features</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">pearson_correlation</span><span class="sh">'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> 
                        <span class="n">threshold</span><span class="o">=</span><span class="n">feature_threshold</span><span class="p">,</span> 
                        <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Pearson Correlation With Diabetes</span><span class="sh">"</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Features</span><span class="sh">"</span><span class="p">,</span> 
                        <span class="n">ylabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Diabetes Correlation</span><span class="sh">"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="p">[</span><span class="n">blue</span><span class="p">,</span> <span class="n">red</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>


<span class="c1"># Select features with correlation value &gt; feature_threshold
</span><span class="n">best_pearson_features</span> <span class="o">=</span> <span class="n">pearson_corr</span><span class="p">[</span><span class="n">pearson_corr</span><span class="p">[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">feature_threshold</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Best correlated features:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">best_pearson_features</span><span class="p">)</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/pearson_diabetes.png" width="800" height="500" />
</center>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Best correlated features:
 ['GenHlth', 'HighBP', 'BMI', 'HighChol', 'Age', 'DiffWalk', 'Income', 'HeartDiseaseorAttack', 
 'Education', 'PhysActivity', 'Stroke', 'CholCheck', 'HvyAlcoholConsump', 'Smoker', 'Veggies']
</code></pre></div></div>

<p>Utilizing three distinct metrics — Mutual Information (MI), Pearson, and Spearman — I constructed a schematic representation of a diagram that illustrates the shared and unique characteristics of each metric.</p>

<p>The diagram can provide a better view of how these metrics intersect and which features emerge as a relevant correlation with the target variable.</p>

<center>
<img src="/assets/img/post4/metrics-venn-diagram.png" width="400" height="400" />
</center>

<p>Using this idea, let take the overlapping sets of features selected by each metric.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mi_set</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">best_mi_features</span><span class="p">)</span>
<span class="n">pearson_set</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">best_pearson_features</span><span class="p">)</span>
<span class="n">spearman_set</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">best_spearman_features</span><span class="p">)</span>

<span class="c1"># Common features across pearson and mutual information
</span><span class="n">pearson_mi_set</span> <span class="o">=</span> <span class="p">(</span><span class="n">pearson_set</span><span class="p">.</span><span class="nf">intersection</span><span class="p">(</span><span class="n">mi_set</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s"> Pearson and MI intersection:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">pearson_mi_set</span><span class="p">)</span>

<span class="c1"># Common features across spearman and mutual information
</span><span class="n">spearman_mi_set</span> <span class="o">=</span> <span class="p">(</span><span class="n">spearman_set</span><span class="p">.</span><span class="nf">intersection</span><span class="p">(</span><span class="n">mi_set</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s"> Spearman and MI intersection:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">spearman_mi_set</span><span class="p">)</span>

<span class="c1"># Common features across spearman and pearson
</span><span class="n">spearman_pearson_set</span> <span class="o">=</span> <span class="p">(</span><span class="n">spearman_set</span><span class="p">.</span><span class="nf">intersection</span><span class="p">(</span><span class="n">pearson_set</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s"> Spearman and Pearson intersection:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">spearman_pearson_set</span><span class="p">)</span>

<span class="c1"># Common features across all three metrics
</span><span class="n">small_feature_set</span> <span class="o">=</span> <span class="n">mi_set</span><span class="p">.</span><span class="nf">intersection</span><span class="p">(</span><span class="n">pearson_set</span><span class="p">).</span><span class="nf">intersection</span><span class="p">(</span><span class="n">spearman_set</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Intersection for all metrics:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">small_feature_set</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Pearson and MI intersection:
 {'BMI', 'HighBP', 'HighChol', 'Income', 'DiffWalk', 'HeartDiseaseorAttack', 'GenHlth', 'Age'}

 Spearman and MI intersection:
 {'BMI', 'HighBP', 'HighChol', 'Income', 'DiffWalk', 'HeartDiseaseorAttack', 'GenHlth', 'Age'}

 Spearman and Pearson intersection:
 {'HvyAlcoholConsump', 'BMI', 'PhysActivity', 'HighBP', 'CholCheck', 'HighChol', 'Income', 
 'DiffWalk', 'HeartDiseaseorAttack', 'GenHlth', 'Age', 'Education', 'Stroke', 'Veggies', 'Smoker'}

Intersection for all metrics:
 {'BMI', 'HighBP', 'HighChol', 'Income', 'DiffWalk', 'HeartDiseaseorAttack', 'GenHlth', 'Age'}
</code></pre></div></div>

<p>From this analyze for the correlations we can conclude that the features <strong>BMI</strong>, <strong>GenHlth</strong>, <strong>Age</strong>, <strong>Income</strong>, <strong>HeartDiseaseorAttack</strong>, <strong>DiffWalk</strong>, <strong>HighBP</strong>, <strong>HighChol</strong> appear to have a higher degree of relevance compare to the target <strong>Diabetes</strong>. All three metrics show a agreement for the relevance of these features.</p>

<p>Now, with a more restricted set of features, it’s essential to closely examine the relationship between the each feature and the target variable based on their frequency. To do this, we can create a frequency table and illustrate the results using a bar plot.</p>

<p>Let’s take the set with the common feature across all three metrics to proceed with the analyze.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Separate in binary and non binary
numerical = ['BMI']
categorical = ['GenHlth', 'Age', 'Income']
binary = ['HighChol','DiffWalk', 'HighBP', 'HeartDiseaseorAttack']
</code></pre></div></div>

<p><strong>Binary</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">binary</span><span class="p">):</span>

    <span class="n">ct</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">crosstab</span><span class="p">(</span><span class="n">df_train_dropped</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span> <span class="n">Y_train_large</span> <span class="p">)</span>
    <span class="n">ct_normalized</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="nf">divide</span><span class="p">(</span><span class="n">ct</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>

    <span class="n">ct_normalized</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="sh">'</span><span class="s">bar</span><span class="sh">'</span><span class="p">,</span> <span class="n">stacked</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="n">blue</span><span class="p">,</span> <span class="n">red</span><span class="p">],</span>
             <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span> <span class="mi">15</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">False</span>  <span class="p">)</span>
    <span class="c1">#sns.barplot(x=ct_normalized.index, y= ct_normalized[1], ax=ax, palette = ['#2D4471', '#BE3232'], alpha = 0.5)
</span>    
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">''</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Percentage %</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span> <span class="mi">18</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xticklabels</span><span class="p">([</span><span class="sh">'</span><span class="s">No</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Yes</span><span class="sh">'</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span> <span class="p">[</span><span class="sh">'</span><span class="s">No Diabetes</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> 
               <span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">upper center</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>

<span class="c1">#for ax in axes.ravel()[len(binary):]:
#    ax.axis('off')
</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/binary_features.png" width="800" height="500" />
</center>

<p>For Binary (categorical) data we can use the chi squared test to quantify how independent two categorical variables are, i.e. if there is an association between the two variables. This is achieved by this statistical test by comparing the observed frequencies to expected frequencies in a contingency table for the classes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cts</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">binary</span><span class="p">:</span>
    <span class="c1"># Compute the contingency table
</span>    <span class="n">ct</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">crosstab</span><span class="p">(</span><span class="n">df_train_dropped</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span> <span class="n">Y_train_large</span><span class="p">)</span>    
    <span class="n">cts</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">ct</span> 
    <span class="c1"># chi-squared test
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">chi2_contingency</span><span class="p">(</span><span class="n">cts</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>

    <span class="c1"># Null hypothesis: There is no association between the two variables
</span>    <span class="nf">if </span><span class="p">(</span><span class="n">p_value</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Significant association between diabetes status and </span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Failed to reject Null Hypothesis</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Significant association between diabetes status and HighChol.
Significant association between diabetes status and DiffWalk.
Significant association between diabetes status and HighBP.
Significant association between diabetes status and HeartDiseaseorAttack.
</code></pre></div></div>

<p>The visualization in a bar plot for <strong>HighChol</strong>, <strong>DiffWalk</strong>, <strong>HighBP</strong> demonstrates a tendency to <strong>Diabetes</strong> when the answer is <strong>Yes</strong>. The visual observation is further supported by the chi-squared test. Thus, it can be inferred that Diabetic patients tend to have higher cholesterol, difficulty walking, and higher blood pressure.</p>

<p>On the other hand, while <strong>HeartDiseaseorAttack</strong> seems to have an association with <strong>Diabetes</strong>, this link is less evident in the visual observation.</p>

<p><strong>Categorical Ordinal</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">categorical</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">categorical</span><span class="p">):</span>

    <span class="n">ct</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">crosstab</span><span class="p">(</span><span class="n">df_train_dropped</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span> <span class="n">Y_train_large</span><span class="p">)</span>
    <span class="n">ct_normalized</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="nf">divide</span><span class="p">(</span><span class="n">ct</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>

    <span class="n">ct_normalized</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="sh">'</span><span class="s">bar</span><span class="sh">'</span><span class="p">,</span> <span class="n">stacked</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="n">blue</span><span class="p">,</span> <span class="n">red</span><span class="p">],</span> 
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Percentage %</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

    <span class="c1"># Allow x-axis tick labels to take on unique values for non-binary variables
</span>    <span class="n">ax</span><span class="p">.</span><span class="nf">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">No Diabetes</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">upper center</span><span class="sh">'</span><span class="p">,</span> 
              <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="c1">#for ax in axes.ravel()[len(non_binary):]:
#    ax.axis('off')
</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/categorical_features.png" width="800" height="300" />
</center>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cts</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">categorical</span><span class="p">:</span>
    <span class="c1"># Compute the contingency table
</span>    <span class="n">ct</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">crosstab</span><span class="p">(</span><span class="n">df_train_dropped</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span> <span class="n">Y_train_large</span><span class="p">)</span>    
    <span class="n">cts</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">ct</span> 
    <span class="c1"># chi-squared test
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">chi2_contingency</span><span class="p">(</span><span class="n">cts</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>

    <span class="c1"># Null hypothesis: There is no association between the two variables
</span>    <span class="nf">if </span><span class="p">(</span><span class="n">p_value</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Significant association between diabetes status and </span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Failed to reject Null Hypothesis</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Significant association between diabetes status and GenHlth.
Significant association between diabetes status and Age.
Significant association between diabetes status and Income.
</code></pre></div></div>

<p>For <strong>GenHlth</strong> it shows a correlation with diabetes. Based on the categories of general health:</p>

<ol>
  <li>excellent : less than $5\%$ has diabetes</li>
  <li>very good : between $15-20 \%$ has diabetes</li>
  <li>good: more than $35\%$ has diabetes</li>
  <li>fair: between $25-30 \%$ has diabetes</li>
  <li>poor: between $10-15 \%$ has diabetes</li>
</ol>

<p>As general health gets worse, the percentage of individuals diagnosed with diabetes increases.</p>

<p>For the <strong>Age</strong> we have the following categories:</p>

<ol>
  <li>18-24</li>
  <li>25-29</li>
  <li>30-34</li>
  <li>35-39</li>
  <li>40-44</li>
  <li>45-49</li>
  <li>50-54</li>
  <li>55-59</li>
  <li>60-64</li>
  <li>65-69</li>
  <li>70-74</li>
  <li>75-79</li>
  <li>more then 80</li>
</ol>

<p>There’s an observed increment in the percentage of individuals with diabetes as age progresses, though there are some exceptions. This is a good sign, because it is in line with known medical data indicating that the risk of diabetes increases with age.</p>

<p>For <strong>Income</strong> , those individuals with the highest income tend to have an increase in the incidence of diabetes compared to lowest income. But is complex to say more than this, because it does not follow a simple linear trend as for <strong>GenHlth</strong> and <strong>Age</strong>.</p>

<p><strong>Numerical</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Divide the dataset into two groups based on Diabetes status
</span><span class="n">df_train_no</span> <span class="o">=</span> <span class="n">df_train_large</span><span class="p">[</span><span class="n">df_train_large</span><span class="p">[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">df__train_yes</span> <span class="o">=</span> <span class="n">df_train_large</span><span class="p">[</span><span class="n">df_train_large</span><span class="p">[</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Select the BMI feature from each group
</span><span class="n">df_no_bmi</span> <span class="o">=</span> <span class="n">df_train_no</span><span class="p">[</span><span class="sh">'</span><span class="s">BMI</span><span class="sh">'</span><span class="p">]</span>
<span class="n">df_yes_bmi</span> <span class="o">=</span> <span class="n">df__train_yes</span><span class="p">[</span><span class="sh">'</span><span class="s">BMI</span><span class="sh">'</span><span class="p">]</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Plotting the distributions
</span><span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">df_yes_bmi</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">red</span> <span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">df_no_bmi</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">blue</span> <span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axvline</span><span class="p">(</span><span class="n">df_yes_bmi</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="n">red</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axvline</span><span class="p">(</span><span class="n">df_no_bmi</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">text</span><span class="p">(</span><span class="n">df_yes_bmi</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="o">-</span><span class="mf">0.007</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">df_yes_bmi</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">color</span><span class="o">=</span><span class="n">red</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="sh">'</span><span class="s">center</span><span class="sh">'</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">text</span><span class="p">(</span><span class="n">df_no_bmi</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="o">-</span><span class="mf">0.007</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">df_no_bmi</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="sh">'</span><span class="s">center</span><span class="sh">'</span><span class="p">)</span>



<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">grid</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">BMI Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">Diabetics</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">No-diabetes</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Plotting the CDFs
</span><span class="n">sns</span><span class="p">.</span><span class="nf">ecdfplot</span><span class="p">(</span><span class="n">df_yes_bmi</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Diabetics</span><span class="sh">"</span><span class="p">,</span> 
             <span class="n">color</span><span class="o">=</span><span class="n">red</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">ecdfplot</span><span class="p">(</span><span class="n">df_no_bmi</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">No-diabetes</span><span class="sh">"</span><span class="p">,</span> 
             <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">grid</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">BMI Cumulative Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/bmi_distribution.png" width="800" height="400" />
</center>

<p>For numerical data, we employ the t-test to compare the means of two groups: BMI for diabetic patients and BMI for non-diabetic patients.  Because the variances of the groups differ, then Welch’s t-test is employed, not assuming equal variances.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Diff of Variances:</span><span class="sh">'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">df_yes_bmi</span><span class="p">.</span><span class="nf">var</span><span class="p">()</span> <span class="o">-</span> <span class="n">df_no_bmi</span><span class="p">.</span><span class="nf">var</span><span class="p">()))</span>
<span class="c1"># the variance of the two groups are different, so we use Welch's t-test
</span><span class="n">_</span><span class="p">,</span> <span class="n">p_value</span>  <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="nf">ttest_ind</span><span class="p">(</span><span class="n">df_yes_bmi</span><span class="p">,</span> <span class="n">df_no_bmi</span><span class="p">,</span> <span class="n">equal_var</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Null hypothesis: The two groups have the same mean
</span><span class="k">if</span> <span class="n">p_value</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>   
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Diabetic and non-diabetic have different BMI</span><span class="sh">'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Failed to reject Null Hypothesis</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Diff of Variances: 16.09348
Diabetic and non-diabetic have different BMI
</code></pre></div></div>

<p>The distributions and cumulative distribution for BMI in diabetic and non-diabetic patients has a significant difference. The BMI distribution for diabetics appears right-skewed, suggesting a tendency towards higher BMI values compared to the more symmetric distribution of non-diabetics. This difference is more evident in their cumulative distributions, which show a significant gap between the curves. This visual observation is further supported by a Welch’s t-test, indicating a significant difference in the mean BMI values of the two groups. Thus, it can be inferred that diabetic patients tend to have higher BMIs than non-diabetics</p>

<h2 id="23-conclusion-from-eda"><strong>2.3 Conclusion from EDA</strong></h2>

<p>After analyzing the data, we found that the best features for training our machine learning model are:</p>

<ul>
  <li>HighBP</li>
  <li>GenHlth</li>
  <li>HighChol</li>
  <li>DiffWalk</li>
  <li>Age</li>
  <li>BMI</li>
  <li>HeartDiseaseorAttack</li>
  <li>Income</li>
</ul>

<p>We saw a clear link between these features and the target variable <strong>Diabetes</strong>. But for the <strong>HeartDiseaseorAttack</strong> and <strong>Income</strong> features, it’s harder to say exactly how they relate or if the relationship is significant for predicting whether an individual has diabetes. All we can infer is that the relationship is not linear. Also, <strong>HeartDiseaseorAttack</strong> is highly centred in one category, and this could have some influence in this relation with the target variable.</p>

<h1 id="3-model-training-and-validation"><strong>3. Model Training and Validation</strong></h1>

<p>Now, with a better understanding of the features and their significance in relation to the target variable <strong>Diabetes</strong>, we can proceed to train a model to evaluate its performance on this set of features identified from the EDA. We consider both a consensus set of features, where all metrics agree, which results in a smaller feature set, and a large set of features selected based on the highest Pearson and mutual information. This approach will allow us to compare the models using different subsets of features, providing a more nuanced understanding of the relevance (or irrelevance) of each feature to the target variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Remove Diabetes, is not necessary anymore
</span><span class="n">boolean_features</span> <span class="o">=</span> <span class="n">boolean_columns</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">boolean_features</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">)</span>
<span class="n">categorical_features</span>  <span class="o">=</span> <span class="n">categorical_columns</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="c1"># Best set of features selected from EDA
</span><span class="n">small_feature_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">small_feature_set</span><span class="p">)</span>

<span class="n">large_feature_set</span> <span class="o">=</span> <span class="n">spearman_pearson_set</span><span class="p">.</span><span class="nf">union</span><span class="p">(</span><span class="n">mi_set</span><span class="p">)</span>
<span class="n">large_feature_list</span><span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">large_feature_set</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">categorical_encoding</span><span class="p">(</span><span class="n">df_train</span> <span class="p">,</span> <span class="n">df_val</span> <span class="p">,</span> <span class="n">df_test</span> <span class="p">,</span><span class="n">features</span> <span class="o">=</span> <span class="bp">None</span> <span class="p">,</span> <span class="n">sparse</span> <span class="o">=</span> <span class="bp">False</span> <span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Function to preprocess data frames based on the selected features.
    
    Parameters:
    - df_train, df_val, df_test: pandas.DataFrame or None
        Training, validation, and test datasets.
    - features: list
        List of features to be selected from the data frames.
        
    Returns:
    - X_train, X_val_small, X_test_small: numpy.ndarray or None
        Processed feature arrays for training, validation, and test datasets.
    - dv: DictVectorizer
        Fitted DictVectorizer object.
    </span><span class="sh">"""</span>
    
    <span class="k">if</span> <span class="n">features</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">df_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="n">features</span><span class="p">].</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">df_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="n">features</span><span class="p">].</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">df_val</span> <span class="o">=</span> <span class="n">df_val</span><span class="p">[</span><span class="n">features</span><span class="p">].</span><span class="nf">copy</span><span class="p">()</span>
    
    <span class="n">dv</span> <span class="o">=</span> <span class="nc">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span> <span class="n">sparse</span><span class="p">)</span>
    <span class="n">dataframes</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">:</span> <span class="n">df_train</span><span class="p">,</span> <span class="sh">'</span><span class="s">val</span><span class="sh">'</span><span class="p">:</span> <span class="n">df_val</span><span class="p">,</span> <span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">:</span> <span class="n">df_test</span><span class="p">}</span>
    <span class="n">transformed_data</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">fitted</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">dataframes</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            
        <span class="n">df_dict</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">to_dict</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="sh">'</span><span class="s">records</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">fitted</span><span class="p">:</span>
            <span class="n">transformed_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">dv</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df_dict</span><span class="p">)</span>
            <span class="n">fitted</span> <span class="o">=</span> <span class="bp">True</span> 
        <span class="k">else</span><span class="p">:</span>
            <span class="n">transformed_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">dv</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">df_dict</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">transformed_data</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">),</span> <span class="n">transformed_data</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">val</span><span class="sh">'</span><span class="p">),</span> <span class="n">transformed_data</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">),</span> <span class="n">dv</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model_train_preprocess</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">df_test</span><span class="p">,</span> <span class="n">selected_features_eda</span> <span class="p">:</span> <span class="nb">list</span><span class="p">,</span>  
                           <span class="n">featurers_to_dtype</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">features_to_scale</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    
    <span class="n">dfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">df_test</span><span class="p">]</span>
    
    <span class="c1"># Standardize features_to_scale if provided
</span>    <span class="k">if</span> <span class="n">features_to_scale</span><span class="p">:</span>
        <span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
        <span class="n">scaler</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">features_to_scale</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">dfs</span><span class="p">:</span>
            <span class="n">df</span><span class="p">[</span><span class="n">features_to_scale</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">features_to_scale</span><span class="p">])</span>
    
    <span class="c1"># Convert data types if dtype_dict is provided
</span>    <span class="k">if</span> <span class="n">featurers_to_dtype</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">dfs</span><span class="p">:</span>
            <span class="n">df</span> <span class="o">=</span> <span class="nf">convert_dtypes</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">featurers_to_dtype</span><span class="p">)</span>

    <span class="c1"># Encode categorical variables for selected features from EDA
</span>    <span class="k">return</span> <span class="nf">categorical_encoding</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">df_test</span><span class="p">,</span> <span class="n">selected_features_eda</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="31-logistic-regression"><strong>3.1 Logistic Regression</strong></h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Best set of features selected from EDA
</span><span class="n">small_feature_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">small_feature_set</span><span class="p">)</span>
<span class="n">large_feature_list</span><span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">spearman_pearson_set</span><span class="p">.</span><span class="nf">union</span><span class="p">(</span><span class="n">mi_set</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s"> Small set of features:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">small_feature_list</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s"> Large set of features:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">large_feature_list</span><span class="p">)</span>

<span class="c1"># Change dtypes for logistic regression
</span><span class="n">dtype_dict_lr</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">bool</span><span class="sh">'</span><span class="p">:</span> <span class="n">boolean_features</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">str</span><span class="sh">'</span><span class="p">:</span> <span class="n">categorical_features</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">BMI</span><span class="sh">"</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># Create a copy of dataframe to modify the dtypes
</span><span class="n">df_train_lr</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">df_val_lr</span> <span class="o">=</span> <span class="n">df_val</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">df_test_lr</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="n">X_train_small</span><span class="p">,</span> <span class="n">X_val_small</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">model_train_preprocess</span><span class="p">(</span> <span class="n">df_train_lr</span><span class="p">,</span> <span class="n">df_val_lr</span><span class="p">,</span> <span class="n">df_test_lr</span><span class="p">,</span> 
                                                           <span class="n">small_feature_list</span><span class="p">,</span>
                                                           <span class="n">dtype_dict_lr</span><span class="p">,</span> <span class="n">features_to_scale</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">BMI</span><span class="sh">'</span><span class="p">])</span>

<span class="n">X_train_large</span><span class="p">,</span> <span class="n">X_val_large</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">model_train_preprocess</span><span class="p">(</span> <span class="n">df_train_lr</span><span class="p">,</span> <span class="n">df_val_lr</span><span class="p">,</span> <span class="n">df_test_lr</span><span class="p">,</span> 
                                                           <span class="n">large_feature_list</span><span class="p">,</span>
                                                           <span class="n">dtype_dict_lr</span><span class="p">,</span> <span class="n">features_to_scale</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">BMI</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Small set of features:
['BMI', 'HighBP', 'HighChol', 'Income', 'DiffWalk', 'HeartDiseaseorAttack', 'GenHlth', 'Age']

Large set of features:
['HvyAlcoholConsump', 'BMI', 'HighChol', 'DiffWalk', 'HeartDiseaseorAttack', 'GenHlth', 
'Age', 'Veggies', 'PhysActivity', 'HighBP', 'CholCheck', 'Income', 'Stroke', 'Education', 'Smoker']
</code></pre></div></div>

<h3 id="311-hyperparameter-tunning-and-validate"><strong>3.1.1 Hyperparameter tunning and validate</strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Grid search cross validation
</span><span class="n">grid</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">C</span><span class="sh">"</span><span class="p">:[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
      <span class="sh">"</span><span class="s">penalty</span><span class="sh">"</span><span class="p">:[</span><span class="sh">"</span><span class="s">l1</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">l2</span><span class="sh">"</span><span class="p">],</span>
      <span class="sh">"</span><span class="s">solver</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span> <span class="sh">'</span><span class="s">liblinear</span><span class="sh">'</span><span class="p">]}</span>

<span class="n">lr_model</span> <span class="o">=</span><span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">lr_model_cv</span><span class="o">=</span><span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">lr_model</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">lr_model_cv</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_large</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Tuned hyperparameters :</span><span class="sh">"</span><span class="p">,</span><span class="n">lr_model_cv</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy of the best hyperparameters :</span><span class="sh">"</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="n">lr_model_cv</span><span class="p">.</span><span class="n">best_score_</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">Y_pred_lr_large</span> <span class="o">=</span> <span class="n">lr_model_cv</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_large</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s"> Accuracy for Large Feature Set:</span><span class="sh">'</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_pred_lr_large</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># 'C': 1, 'penalty': 'l2', 'solver': 'liblinear'
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Tuned hyperparameters : {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}

Accuracy of the best hyperparameters : 0.744

Accuracy for Large Feature Set: 0.748
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train and validate baseline features
</span><span class="n">lr_model</span><span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="sh">'</span><span class="s">liblinear</span><span class="sh">'</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span> <span class="sh">'</span><span class="s">l2</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lr_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_small</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred_lr_small</span> <span class="o">=</span> <span class="n">lr_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_small</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy for Small Feature Set:</span><span class="sh">'</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_pred_lr_small</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy for Small Feature Set: 0.744
</code></pre></div></div>

<h3 id="312-feature-importance"><strong>3.1.2 Feature importance</strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">feature_elimination</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">metric_func</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">base_metric</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Function to perform feature elimination based on the given model and metric function.

    Parameters:
    - df_train, df_val: pandas.DataFrame
        Training and validation datasets.
    - Y_train, Y_val: pandas.Series or numpy.ndarray
        Target values for training and validation datasets.
    - model: scikit-learn estimator
        The machine learning model to be trained.
    - metric_func: callable
        The metric function to evaluate model performance.
    - base_metric: float
        The base accuracy or metric value to compare against.

    Returns:
    - df_feature_metrics: pandas.DataFrame
        DataFrame showing the impact of each feature</span><span class="sh">'</span><span class="s">s elimination on model performance.
    </span><span class="sh">"""</span>
    
    <span class="n">eliminated_features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">metric_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">metric_diff_list</span> <span class="o">=</span> <span class="p">[]</span>   <span class="c1"># to store the difference
</span>    <span class="n">metric_name</span> <span class="o">=</span> <span class="n">metric_func</span><span class="p">.</span><span class="n">__name__</span>

    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>

        <span class="n">df_train_drop</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">df_val_drop</span> <span class="o">=</span> <span class="n">df_val</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">X_train_small</span><span class="p">,</span> <span class="n">X_val_small</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">categorical_encoding</span><span class="p">(</span><span class="n">df_train_drop</span><span class="p">,</span> <span class="n">df_val_drop</span><span class="p">,</span> <span class="n">df_val_drop</span><span class="p">)</span>

        <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_small</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
        <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_small</span><span class="p">)</span>
        <span class="n">metric_score</span> <span class="o">=</span> <span class="nf">metric_func</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)</span>
        
        <span class="c1"># Store results in lists
</span>        <span class="n">eliminated_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
        <span class="n">metric_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">metric_score</span><span class="p">)</span>
        <span class="n">metric_diff_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">base_metric</span> <span class="o">-</span> <span class="n">metric_score</span><span class="p">))</span>  <span class="c1"># compute the difference
</span>
    <span class="n">df_feature_metrics</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span> 
        <span class="sh">'</span><span class="s">eliminated_feature</span><span class="sh">'</span><span class="p">:</span> <span class="n">eliminated_features</span><span class="p">,</span>
        <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="sh">'</span><span class="p">:</span> <span class="n">metric_list</span><span class="p">,</span>
        <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s">_diff</span><span class="sh">'</span><span class="p">:</span> <span class="n">metric_diff_list</span>  <span class="c1"># add the difference as a column
</span>    <span class="p">})</span>\
    <span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s">_diff</span><span class="sh">'</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">df_feature_metrics</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr_model</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="sh">'</span><span class="s">liblinear</span><span class="sh">'</span><span class="p">,</span>  <span class="n">penalty</span><span class="o">=</span> <span class="sh">'</span><span class="s">l1</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">feature_importance</span> <span class="o">=</span> <span class="nf">feature_elimination</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span>
                                   <span class="n">lr_model</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">large_feature_list</span><span class="p">,</span> 
                                   <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_pred_lr_large</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)</span> <span class="p">)</span>

<span class="c1">#display(feature_importance)
</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">importance_threshold</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">red</span> <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;=</span> <span class="n">importance_threshold</span> <span class="k">else</span> <span class="n">blue</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">feature_importance</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy_score_diff</span><span class="sh">'</span><span class="p">]]</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">eliminated_feature</span><span class="sh">"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">accuracy_score_diff</span><span class="sh">"</span><span class="p">,</span> 
            <span class="n">data</span><span class="o">=</span><span class="n">feature_importance</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">set_xticklabels</span><span class="p">(</span><span class="n">feature_importance</span><span class="p">[</span><span class="sh">"</span><span class="s">eliminated_feature</span><span class="sh">"</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy Difference (Importance)</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Features</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span> 
<span class="n">ax</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature Elimination </span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/feature_elimination.png" width="800" height="400" />
</center>

<p>The table indicates that the features <strong>GenHlth</strong>, <strong>BMI</strong>, <strong>Age</strong>, <strong>HighBP</strong>, <strong>HighChol</strong> are relevant for the model’s accuracy. Removing any of these specific features leads to a large decline in accuracy compared to the others features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Save most important features for logistic regression
</span><span class="n">feature_importance_lr</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span>
  <span class="n">feature_importance</span><span class="p">[</span><span class="n">feature_importance</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy_score_diff</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">importance_threshold</span><span class="p">][</span><span class="sh">'</span><span class="s">eliminated_feature</span><span class="sh">'</span><span class="p">]</span>
    <span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s">Features importance Logistic Regression:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">feature_importance_lr</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Features importance Logistic Regression:
 {'BMI', 'HighBP', 'HighChol', 'GenHlth', 'Age'}
</code></pre></div></div>

<h3 id="313-metrics"><strong>3.1.3 Metrics</strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">column_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Value</span><span class="sh">'</span><span class="p">):</span>
    <span class="c1"># Convert predicted probabilities to binary predictions
</span>    <span class="c1">#y_pred = [1 if prob &gt;= 0.5 else 0 for prob in y_pred_prob]
</span>    
    <span class="c1"># Calculate metrics
</span>    <span class="n">precision</span> <span class="o">=</span> <span class="nf">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">recall</span> <span class="o">=</span> <span class="nf">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">auc</span> <span class="o">=</span> <span class="nf">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    
    <span class="c1"># Create DataFrame
</span>    <span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
        <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">Precision</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Recall</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">AUC</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">F1 Score</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">column_name</span><span class="p">:</span> <span class="p">[</span><span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">f1</span><span class="p">]</span>
    <span class="p">}).</span><span class="nf">set_index</span><span class="p">(</span><span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">metrics_df</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr_metrics_large</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_lr_large</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr_large_set</span><span class="sh">'</span><span class="p">)</span>
<span class="n">lr_metrics_small</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_lr_small</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr_small_set</span><span class="sh">'</span><span class="p">)</span>

<span class="n">lr_metrics</span> <span class="o">=</span> <span class="n">lr_metrics_large</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">lr_metrics_small</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>


<span class="nf">display</span><span class="p">(</span><span class="n">lr_metrics</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>lr_large_set</th>
      <th>lr_small_set</th>
    </tr>
    <tr>
      <th>metrics</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Precision</th>
      <td>0.738683</td>
      <td>0.735145</td>
    </tr>
    <tr>
      <th>Recall</th>
      <td>0.783458</td>
      <td>0.781476</td>
    </tr>
    <tr>
      <th>AUC</th>
      <td>0.746788</td>
      <td>0.743500</td>
    </tr>
    <tr>
      <th>F1 Score</th>
      <td>0.760412</td>
      <td>0.757603</td>
    </tr>
  </tbody>
</table>
</div>

<p>Given that the smaller feature set provides almost equivalent performance to the larger set, specially for the F1 score and the recall metrics, we can conclude that the large set of features has some irrelevant features for the predictive power of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">Y_true</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">xy_legends</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Plot the confusion matrix for given true and predicted labels.

    Parameters:
    - Y_true: Actual labels
    - Y_pred: Predicted labels
    - title: Title for the plot
    - ax: Axis object to plot on
    </span><span class="sh">"""</span>
    <span class="n">cm</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">Y_true</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>
    <span class="n">cm_norm</span> <span class="o">=</span> <span class="n">cm</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="n">cm</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">cm_norm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">'</span><span class="s">.2%</span><span class="sh">'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">coolwarm</span><span class="sh">'</span><span class="p">,</span>
                <span class="n">xticklabels</span><span class="o">=</span> <span class="n">xy_legends</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span> <span class="n">xy_legends</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Predicted labels</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True labels</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">Large Feature Set For Logistic Regression</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_lr_large</span><span class="p">,</span>  <span class="c1"># Predictions using large feature set
</span>    <span class="sh">"</span><span class="s">Small Feature Set For Logistic Regression</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_lr_small</span>   <span class="c1"># Predictions using small feature set
</span><span class="p">}</span>

<span class="c1"># Create plots
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">predictions</span><span class="p">.</span><span class="nf">items</span><span class="p">()):</span>
    <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">xy_legends</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">No Diabetes</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/confusion_lr.png" width="800" height="400" />
</center>

<p>The models exhibit a demonstrates a reasonably ability in correctly identifying individuals with and without diabetes, with True Positive (sensitivity) rates of 78.35% for the larger feature set and 78.15% for the smaller feature set, and True Negative (specificity) rates of 71.01% and 70.55% respectively. However, in a potential clinical setting, the observed False Negative rates of 21.65% for the larger set and 21.85% for the smaller set could be of problematic, as they represent the proportion of actual diabetic patients who were not identified by the model. Same for the False Positive rates.</p>

<p>While the balance between precision and recall suggests that the models perform reasonably well for a general application, the consequences of false predictions in a medical context could be bad.</p>

<h2 id="32-decision-tree-and-random-forest"><strong>3.2 Decision Tree and Random Forest</strong></h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_train_trees</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">df_val_trees</span> <span class="o">=</span> <span class="n">df_val</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="n">dtype_dict_trees</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">int32</span><span class="sh">'</span><span class="p">:</span> <span class="n">categorical_features</span> <span class="o">+</span> <span class="n">boolean_features</span>
<span class="p">}</span>

<span class="n">df_train_trees</span> <span class="o">=</span> <span class="nf">convert_dtypes</span><span class="p">(</span><span class="n">df_train_trees</span><span class="p">,</span> <span class="n">dtype_dict_trees</span><span class="p">)</span>
<span class="n">df_val_trees</span> <span class="o">=</span> <span class="nf">convert_dtypes</span><span class="p">(</span><span class="n">df_val_trees</span><span class="p">,</span> <span class="n">dtype_dict_trees</span><span class="p">)</span>


<span class="c1"># For decision three is need only to select the feature matrix from dataframe
</span><span class="n">X_train_small</span><span class="p">,</span> <span class="n">X_val_small</span> <span class="o">=</span>    <span class="n">df_train_trees</span><span class="p">[</span><span class="n">small_feature_list</span><span class="p">].</span><span class="n">values</span><span class="p">,</span>\
                                <span class="n">df_val_trees</span><span class="p">[</span><span class="n">small_feature_list</span><span class="p">].</span><span class="n">values</span>
                                
<span class="n">X_train_large</span><span class="p">,</span> <span class="n">X_val_large</span> <span class="o">=</span>    <span class="n">df_train_trees</span><span class="p">[</span><span class="n">large_feature_list</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> \
                                <span class="n">df_val_trees</span><span class="p">[</span><span class="n">large_feature_list</span><span class="p">].</span><span class="n">values</span>
</code></pre></div></div>

<h3 id="321-hyperparameter-tunning-and-validate"><strong>3.2.1 Hyperparameter Tunning And Validate</strong></h3>

<p><strong>Decision tree</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Decision Tree
</span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span> <span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span> <span class="p">],</span>
    <span class="sh">'</span><span class="s">min_samples_split</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">min_samples_leaf</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">dt</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">)</span>
<span class="n">grid_search</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_large</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="c1"># Get best parameters and best score
</span><span class="n">best_params</span> <span class="o">=</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_params_</span>
<span class="n">best_score</span> <span class="o">=</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_score_</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Best Parameters: </span><span class="si">{</span><span class="n">best_params</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Best Accuracy Score: </span><span class="si">{</span><span class="n">best_score</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Predict using best estimator
</span><span class="n">Y_pred_dt_large</span> <span class="o">=</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_large</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s"> Accuracy for Large Feature Set:</span><span class="sh">'</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_pred_dt_large</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># 'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2}

Best Accuracy Score: 0.7295637043271721

Accuracy for Large Feature Set: 0.732
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt_model_small</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                                        <span class="n">min_samples_split</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">criterion</span> <span class="o">=</span> <span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dt_model_small</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_small</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred_dt_small</span> <span class="o">=</span> <span class="n">dt_model_small</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_small</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy for Small Feature Set::</span><span class="sh">'</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_pred_dt_small</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy for Small Feature Set:: 0.735
</code></pre></div></div>

<p><strong>Random Forest</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the hyperparameter grid
</span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">min_samples_split</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">min_samples_leaf</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">criterion</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">n_jobs</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  
<span class="p">}</span>

<span class="c1"># Instantiate the RandomForestClassifier
</span><span class="n">rf_model</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>

<span class="c1"># Perform GridSearch
</span><span class="n">grid_search</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">rf_model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">)</span>
<span class="n">grid_search</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_large</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="c1"># Retrieve best parameters and best score
</span><span class="n">best_params</span> <span class="o">=</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_params_</span>
<span class="n">best_score</span> <span class="o">=</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_score_</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Best Parameters: </span><span class="si">{</span><span class="n">best_params</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Best Accuracy Score: </span><span class="si">{</span><span class="n">best_score</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Predict using the best estimator
</span><span class="n">Y_pred_rf_large</span> <span class="o">=</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_large</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s"> Large Model Accuracy:</span><span class="sh">'</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># criterion': 'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 15, 
# 'min_samples_split': 5, 'n_estimators': 60, 'n_jobs': -1
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Best Parameters: {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 20, 
                    'min_samples_split': 5, 'n_estimators': 50, 'n_jobs': -1}

Best Accuracy Score: 0.745

Large Model Accuracy: 0.744
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rf_model</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                                  <span class="n">n_estimators</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rf_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_small</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred_rf_small</span> <span class="o">=</span> <span class="n">rf_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_small</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Small Model Accuracy:</span><span class="sh">'</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_pred_rf_small</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Small Model Accuracy: 0.747
</code></pre></div></div>

<h3 id="322-feature-importance"><strong>3.2.2 Feature Importance</strong></h3>

<p>Feature importance in most tree-ensembles is calculated based an importance score. The importance score is a measure of how often the feature was selected for splitting and how much gain in purity was achieved as a result of the selection.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt_model</span><span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_split</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                                  <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">criterion</span> <span class="o">=</span> <span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">)</span>

<span class="n">dt_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_large</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="n">rf_model</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                                  <span class="n">n_estimators</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">rf_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_large</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="c1"># Extract feature importance
</span><span class="n">importances_dt</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span> <span class="n">dt_model</span><span class="p">.</span><span class="n">feature_importances_</span> <span class="p">,</span> 
                              <span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Index</span><span class="p">(</span><span class="n">large_feature_list</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">),</span>
                              <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">feature_importance</span><span class="sh">'</span><span class="p">])</span>\
                              <span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="sh">'</span><span class="s">feature_importance</span><span class="sh">'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>\
                              <span class="p">.</span><span class="nf">reset_index</span><span class="p">()</span>

<span class="n">importances_rf</span> <span class="o">=</span>  <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span> <span class="n">rf_model</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> 
                                <span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Index</span><span class="p">(</span><span class="n">large_feature_list</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">),</span>
                                <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">feature_importance</span><span class="sh">'</span><span class="p">])</span>\
                                <span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="sh">'</span><span class="s">feature_importance</span><span class="sh">'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>\
                                <span class="p">.</span><span class="nf">reset_index</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

<span class="c1"># Plot Decision Tree Feature Importance
</span><span class="n">feature_threshold</span> <span class="o">=</span> <span class="mf">0.04</span>
<span class="nf">plot_feature_importance</span><span class="p">(</span> <span class="n">df</span><span class="o">=</span><span class="n">importances_dt</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">feature_importance</span><span class="sh">'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                         <span class="n">threshold</span><span class="o">=</span><span class="n">feature_threshold</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Decision Tree</span><span class="sh">'</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="p">[</span><span class="n">blue</span><span class="p">,</span> <span class="n">red</span><span class="p">])</span>

<span class="c1"># Plot Random Forest Feature Importance
</span><span class="nf">plot_feature_importance</span><span class="p">(</span> <span class="n">df</span><span class="o">=</span><span class="n">importances_rf</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">feature_importance</span><span class="sh">'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
                         <span class="n">threshold</span><span class="o">=</span><span class="n">feature_threshold</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Random Forest</span><span class="sh">'</span><span class="p">,</span> <span class="n">palette</span> <span class="o">=</span> <span class="p">[</span><span class="n">blue</span><span class="p">,</span> <span class="n">red</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/dt_rf_feature_importance.png" width="800" height="800" />
</center>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_importance_dt</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span>
    <span class="n">importances_dt</span><span class="p">[</span><span class="n">importances_dt</span><span class="p">[</span><span class="sh">'</span><span class="s">feature_importance</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">feature_threshold</span><span class="p">][</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">])</span>
<span class="n">feature_importance_rf</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span>
    <span class="n">importances_rf</span><span class="p">[</span><span class="n">importances_rf</span><span class="p">[</span><span class="sh">'</span><span class="s">feature_importance</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">feature_threshold</span><span class="p">][</span><span class="sh">'</span><span class="s">features</span><span class="sh">'</span><span class="p">])</span>

<span class="n">best_feature_set</span> <span class="o">=</span> <span class="n">feature_importance_dt</span><span class="p">.</span><span class="nf">intersection</span><span class="p">(</span><span class="n">feature_importance_rf</span><span class="p">)</span>\
                                        <span class="p">.</span><span class="nf">intersection</span><span class="p">(</span><span class="n">feature_importance_lr</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Features importance Decision Tree:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">feature_importance_dt</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s">Features importance Random Forest:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">feature_importance_rf</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s">Features importance Logistic Regression:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">feature_importance_lr</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s"> Agreement between the three models:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">best_feature_set</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Features importance Decision Tree:
 {'BMI', 'HighBP', 'HighChol', 'GenHlth', 'Age'}

Features importance Random Forest:
 {'BMI', 'HighBP', 'HighChol', 'Income', 'DiffWalk', 'GenHlth', 'Age'}

Features importance Logistic Regression:
 {'BMI', 'HighBP', 'HighChol', 'GenHlth', 'Age'}

 Agreement between the three models:
 {'BMI', 'HighBP', 'HighChol', 'GenHlth', 'Age'}
</code></pre></div></div>

<h3 id="323-metrics"><strong>3.2.3 Metrics</strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt_metrics_large</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_dt_large</span><span class="p">,</span> <span class="sh">'</span><span class="s">dt_large_set</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dt_metrics_small</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_dt_small</span><span class="p">,</span> <span class="sh">'</span><span class="s">dt_small_set</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dt_metrics</span> <span class="o">=</span> <span class="n">dt_metrics_large</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">dt_metrics_small</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>

<span class="n">rf_metrics_large</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_rf_large</span><span class="p">,</span> <span class="sh">'</span><span class="s">rf_large_set</span><span class="sh">'</span><span class="p">)</span>
<span class="n">rf_metrics_small</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_rf_small</span><span class="p">,</span> <span class="sh">'</span><span class="s">rf_small_set</span><span class="sh">'</span><span class="p">)</span>
<span class="n">rf_metrics</span> <span class="o">=</span> <span class="n">rf_metrics_large</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">rf_metrics_small</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>

<span class="n">trees_metrics</span> <span class="o">=</span> <span class="n">dt_metrics</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">rf_metrics</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">display</span><span class="p">(</span><span class="n">trees_metrics</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dt_large_set</th>
      <th>dt_small_set</th>
      <th>rf_large_set</th>
      <th>rf_small_set</th>
    </tr>
    <tr>
      <th>metrics</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Precision</th>
      <td>0.724642</td>
      <td>0.728750</td>
      <td>0.732304</td>
      <td>0.731363</td>
    </tr>
    <tr>
      <th>Recall</th>
      <td>0.765897</td>
      <td>0.768588</td>
      <td>0.792664</td>
      <td>0.798895</td>
    </tr>
    <tr>
      <th>AUC</th>
      <td>0.730749</td>
      <td>0.734687</td>
      <td>0.744799</td>
      <td>0.745989</td>
    </tr>
    <tr>
      <th>F1 Score</th>
      <td>0.744698</td>
      <td>0.748139</td>
      <td>0.761289</td>
      <td>0.763639</td>
    </tr>
  </tbody>
</table>
</div>

<p>As we concluded for the logistic regression model, the smaller feature set provides almost equivalent performance to the larger set, specially for the F1 score and the recall metrics. which again suggests that the smaller feature set may contain the most relevant features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">Large Feature Set For Decision Tree </span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_dt_large</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">Small Feature Set For Decision Tree</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_dt_small</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Large Feature Set For Random Forest</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_rf_large</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Small Feature Set For Random Forest</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_rf_small</span>  

<span class="p">}</span>

<span class="c1"># Create plots
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">((</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">predictions</span><span class="p">.</span><span class="nf">items</span><span class="p">()):</span>
    <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">xy_legends</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">No Diabetes</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/confusion_dt.png" width="800" height="600" />
</center>

<p>The Decision Tree and Random Forest models perform similarly for the different sets, with Random Forest showing a better predictive power in identifying true positives (correctly classify a diabetic individual) with 79.66%. The false negative rates are relatively low for all models, which is critical for a clinical test. This also confirm a suspected about redundancy in the larger feature set.</p>

<h2 id="33-selecting-model-and-best-set-of-features"><strong>3.3 Selecting Model And Best Set Of Features</strong></h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Best set of features from the models
</span><span class="n">best_features_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">best_feature_set</span><span class="p">)</span>

<span class="c1"># Compare the difference between the sets of features
</span><span class="n">diff_feature_set</span> <span class="o">=</span> <span class="n">small_feature_set</span><span class="p">.</span><span class="nf">difference</span><span class="p">(</span><span class="n">best_feature_set</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Features in small set but not in best set:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">diff_feature_set</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Features in small set but not in best set:
 {'Income', 'DiffWalk', 'HeartDiseaseorAttack'}
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># Best set of features from the models
</span><span class="n">best_features_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">best_feature_set</span><span class="p">)</span>

<span class="n">X_train_lr</span><span class="p">,</span> <span class="n">X_val_lr</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">model_train_preprocess</span><span class="p">(</span><span class="n">df_train_lr</span><span class="p">,</span> <span class="n">df_val_lr</span><span class="p">,</span> <span class="n">df_test_lr</span><span class="p">,</span> <span class="n">best_features_list</span><span class="p">,</span>
                                                                      <span class="n">dtype_dict_lr</span><span class="p">,</span> <span class="n">features_to_scale</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">BMI</span><span class="sh">'</span><span class="p">])</span>

<span class="n">X_train_trees</span><span class="p">,</span> <span class="n">X_val_trees</span> <span class="o">=</span> <span class="n">df_train_trees</span><span class="p">[</span><span class="n">best_features_list</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_val_trees</span><span class="p">[</span><span class="n">best_features_list</span><span class="p">].</span><span class="n">values</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Logistic Regression
</span><span class="n">lr_model</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="sh">'</span><span class="s">liblinear</span><span class="sh">'</span><span class="p">,</span> <span class="n">penalty</span> <span class="o">=</span> <span class="sh">'</span><span class="s">l2</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">lr_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_lr</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred_lr_best</span> <span class="o">=</span> <span class="n">lr_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_lr</span><span class="p">)</span>

<span class="c1">#  Decision Tree
</span><span class="n">dt_model</span><span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                                  <span class="n">min_samples_split</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">criterion</span> <span class="o">=</span> <span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">,</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span> <span class="p">)</span>
<span class="n">dt_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_trees</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred_dt_best</span> <span class="o">=</span> <span class="n">dt_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_trees</span><span class="p">)</span>


<span class="c1"># Random Forest
</span><span class="n">rf_model</span><span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                                         <span class="n">n_estimators</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span><span class="n">criterion</span> <span class="o">=</span> <span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span> <span class="p">,</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rf_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_trees</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred_rf_best</span> <span class="o">=</span> <span class="n">rf_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_trees</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">Large Feature Set For Logistic Regression </span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_lr_large</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">Small Feature Set For Logistic Regression</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_lr_small</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Best Feature Set For Logistic Regression</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_lr_best</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Large Feature Set For Decision Tree </span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_dt_large</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">Small Feature Set For Decision Tree</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_dt_small</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Best Feature Set For Decision Tree</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_dt_best</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Large Feature Set For Random Forest</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_rf_large</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Small Feature Set For Random Forest</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_rf_small</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Best Feature Set For Random Forest</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y_pred_rf_best</span>
<span class="p">}</span>

<span class="c1"># Create plots
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">predictions</span><span class="p">.</span><span class="nf">items</span><span class="p">()):</span>
    <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">xy_legends</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">No Diabetes</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/confusion_lr_dt_rf.png" width="800" height="800" />
</center>

<p>The best feature set consistently outperforms or matches the performance of the large and small feature sets in predicting both classes across all models! This is very good news, because implies that feature selection has been effective in improving model performance, reducing the complexity of the model.</p>

<p>One conclusion that we could drawn from this analyze is that the minor differences between the feature set performances may suggest that the models are somewhat robust to the features selected, or it may indicate that all sets contain the most critical features needed for making predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr_metrics_best</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_lr_best</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr_best_set</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dt_metrics_best</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_dt_best</span><span class="p">,</span> <span class="sh">'</span><span class="s">dt_best_set</span><span class="sh">'</span><span class="p">)</span>
<span class="n">rf_metrics_best</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_rf_best</span><span class="p">,</span> <span class="sh">'</span><span class="s">rf_best_set</span><span class="sh">'</span><span class="p">)</span>

<span class="n">metrics_best</span> <span class="o">=</span> <span class="n">lr_metrics_best</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">dt_metrics_best</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>\
                              <span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">rf_metrics_best</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>

<span class="n">lr_metrics_large</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_lr_large</span><span class="p">,</span> <span class="sh">'</span><span class="s">lr_large_set</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dt_metrics_large</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_dt_large</span><span class="p">,</span> <span class="sh">'</span><span class="s">dt_large_set</span><span class="sh">'</span><span class="p">)</span>
<span class="n">rf_metrics_large</span> <span class="o">=</span> <span class="nf">classification_metrics</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_rf_large</span><span class="p">,</span> <span class="sh">'</span><span class="s">rf_large_set</span><span class="sh">'</span><span class="p">)</span>

<span class="n">metrics_large</span> <span class="o">=</span> <span class="n">lr_metrics_large</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">dt_metrics_large</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>\
                                <span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">rf_metrics_large</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>

<span class="n">metrics_small</span> <span class="o">=</span> <span class="n">lr_metrics_small</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">dt_metrics_small</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>\
                                <span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">rf_metrics_small</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>

<span class="n">all_metrics</span> <span class="o">=</span> <span class="n">metrics_best</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">metrics_large</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>\
                          <span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">metrics_small</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="sh">'</span><span class="s">metrics</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="sh">'</span><span class="s">outer</span><span class="sh">'</span><span class="p">)</span>
                           
<span class="nf">display</span><span class="p">(</span><span class="n">all_metrics</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>lr_best_set</th>
      <th>dt_best_set</th>
      <th>rf_best_set</th>
      <th>lr_large_set</th>
      <th>dt_large_set</th>
      <th>rf_large_set</th>
      <th>lr_small_set</th>
      <th>dt_small_set</th>
      <th>rf_small_set</th>
    </tr>
    <tr>
      <th>metrics</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Precision</th>
      <td>0.731688</td>
      <td>0.722201</td>
      <td>0.724752</td>
      <td>0.738683</td>
      <td>0.724642</td>
      <td>0.732304</td>
      <td>0.735145</td>
      <td>0.728750</td>
      <td>0.731363</td>
    </tr>
    <tr>
      <th>Recall</th>
      <td>0.780909</td>
      <td>0.786433</td>
      <td>0.795780</td>
      <td>0.783458</td>
      <td>0.765897</td>
      <td>0.792664</td>
      <td>0.781476</td>
      <td>0.768588</td>
      <td>0.798895</td>
    </tr>
    <tr>
      <th>AUC</th>
      <td>0.740699</td>
      <td>0.735017</td>
      <td>0.739839</td>
      <td>0.746788</td>
      <td>0.730749</td>
      <td>0.744799</td>
      <td>0.743500</td>
      <td>0.734687</td>
      <td>0.745989</td>
    </tr>
    <tr>
      <th>F1 Score</th>
      <td>0.755498</td>
      <td>0.752949</td>
      <td>0.758607</td>
      <td>0.760412</td>
      <td>0.744698</td>
      <td>0.761289</td>
      <td>0.757603</td>
      <td>0.748139</td>
      <td>0.763639</td>
    </tr>
  </tbody>
</table>
</div>

<p>The minor differences between the metrics become clearer upon closer inspection. For example, the AUC values are quite close across all models, suggesting a consistent ability to distinguish between positive and negative classes. Furthermore, the F1 Score and Recall metrics indicate that the models excel at accurately identifying relevant cases with a low count of false negatives.</p>

<p>The most balanced model in terms of complexity and predictive power is the Random Forest with the best feature set, which contains only five features for this model.</p>

<h1 id="4-model-evaluation-on-test-set"><strong>4. Model Evaluation on Test Set</strong></h1>

<p>Next, we’ll evaluate the models on the test set. This step is important to confirm that the models’ performance is consistent with the validation results and to demonstrate their predictive stability on unseen data.</p>

<p><strong>Logistic Regression</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_features_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">best_feature_set</span><span class="p">)</span>

<span class="n">X_train_lr</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">X_test_lr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">model_train_preprocess</span><span class="p">(</span><span class="n">df_train_lr</span><span class="p">,</span> <span class="n">df_val_lr</span><span class="p">,</span> <span class="n">df_test_lr</span><span class="p">,</span> <span class="n">best_features_list</span><span class="p">,</span>
                                                    <span class="n">dtype_dict_lr</span><span class="p">,</span> <span class="n">features_to_scale</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">BMI</span><span class="sh">'</span><span class="p">])</span>


<span class="n">lr_model</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="sh">'</span><span class="s">liblinear</span><span class="sh">'</span><span class="p">,</span> <span class="n">penalty</span> <span class="o">=</span> <span class="sh">'</span><span class="s">l2</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">lr_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_lr</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred_lr</span> <span class="o">=</span> <span class="n">lr_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test_lr</span><span class="p">)</span>

<span class="n">accuracy_lr</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred_lr</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy for Logistic Regression on test set:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">accuracy_lr</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy for Logistic Regression on test set:
 0.738
</code></pre></div></div>

<p><strong>Decision Tree</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Best set of features selected from EDA
</span><span class="n">best_features_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">best_feature_set</span><span class="p">)</span>

<span class="n">df_train_trees</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">df_test_trees</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="n">dtype_dict_trees</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">int32</span><span class="sh">'</span><span class="p">:</span> <span class="n">categorical_features</span> <span class="o">+</span> <span class="n">boolean_features</span>
<span class="p">}</span>

<span class="n">df_train_trees</span> <span class="o">=</span> <span class="nf">convert_dtypes</span><span class="p">(</span><span class="n">df_train_trees</span><span class="p">,</span> <span class="n">dtype_dict_trees</span><span class="p">)</span>
<span class="n">df_test_trees</span> <span class="o">=</span> <span class="nf">convert_dtypes</span><span class="p">(</span><span class="n">df_test_trees</span><span class="p">,</span> <span class="n">dtype_dict_trees</span><span class="p">)</span>

<span class="c1"># For decision three is need only to select the feature matrix from dataframe
</span><span class="n">X_train_best</span><span class="p">,</span> <span class="n">X_test_best</span> <span class="o">=</span> <span class="n">df_train_trees</span><span class="p">[</span><span class="n">best_features_list</span><span class="p">].</span><span class="n">values</span><span class="p">,</span><span class="n">df_test_trees</span><span class="p">[</span><span class="n">best_features_list</span><span class="p">].</span><span class="n">values</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt_model</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                                        <span class="n">min_samples_split</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">criterion</span> <span class="o">=</span> <span class="sh">'</span><span class="s">gini</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dt_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_trees</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred_dt</span> <span class="o">=</span> <span class="n">dt_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test_best</span><span class="p">)</span>

<span class="n">accuracy_dt</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred_dt</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy for Decision Tree on test set:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">accuracy_dt</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy for Decision Tree on test set:
 0.731
</code></pre></div></div>

<p><strong>Random Forest</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rf_model</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                                  <span class="n">n_estimators</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">rf_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_trees</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_pred_rf</span> <span class="o">=</span> <span class="n">rf_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test_best</span><span class="p">)</span>

<span class="n">accuracy_rf</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred_rf</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy for Random Forest on test set:</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">accuracy_rf</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy for Random Forest on test set:
 0.734
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">predictions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">Logistic Regression On Validation Set</span><span class="sh">"</span><span class="p">:</span> <span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_lr_best</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Logistic Regression On Test Set</span><span class="sh">"</span><span class="p">:</span> <span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred_lr</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Decision Tree On Validation Set</span><span class="sh">"</span><span class="p">:</span> <span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_dt_best</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Decision Tree On Test Set</span><span class="sh">"</span><span class="p">:</span> <span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred_dt</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Random Forest On Validation Set</span><span class="sh">"</span><span class="p">:</span> <span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">Y_pred_rf_best</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Random Forest On Test Set</span><span class="sh">"</span><span class="p">:</span> <span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred_rf</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">title</span><span class="p">,</span>  <span class="p">(</span><span class="n">Y_true</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">))</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">predictions</span><span class="p">.</span><span class="nf">items</span><span class="p">()):</span>
        <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">Y_true</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">xy_legends</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">No Diabetes</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Diabetes</span><span class="sh">'</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<center>
<img src="/assets/img/post4/confusion_dt_rf_lr_test.png" width="800" height="1000" />
</center>

<p>Each model shows a level of consistency between the validation set and test set, with only slight variations in the percentages of true positives (individuals correctly predicted with diabetes) and true negatives (individuals correctly predicted without diabetes), as well as false positives and false negatives.  This suggests that the models are likely generalizing well and not overfitting to the validation data.</p>

<p>Also, we can note that the Random Forest model shows a high recall with a slight advantage in minimizing false positives (incorrectly predicts diabetes in an individual ), it could be considered the most stable model among the three, especially if the priority is to minimize false negatives.</p>

<h1 id="5-conclusions"><strong>5. Conclusions</strong></h1>

<p>From the exploratory data analysis (EDA), we identified the possibility of dropping some redundant features from the dataset out of the initial 22, leading to two subsets: the ‘small feature set’ (with features agreed upon by all correlation metrics) and the ‘large feature set’ (a union of features selected by both Mutual Information and Pearson Correlation).</p>

<p>In the Model Training and Validation section we showed that both feature sets produced similar outcomes across Logistic Regression, Decision Tree, and Random Forest models, with only slight variances in accuracy and other metrics such as F1 Score, Precision, Recall, and AUC. The Features selected by the models shows to be a even smaller set of feature, with only five features, that demonstrates also a high predictive power with only a marginal difference from the other features set.</p>

<p>The challenge in reducing the false negative rate (individuals wrongly classified with diabetes) from the different models may be due the possible skewed class distribution of the features <strong>CholCHeck</strong>, <strong>Stroke</strong>, <strong>HeartDiseaseorAttack</strong>, <strong>Vaggies</strong>, <strong>HvyAlcoholConsump</strong>,  <strong>AnyHealthcare</strong>, <strong>NoDocbcCost</strong>, as noted previously in the EDA. One idea for improvement for the dataset in a future work may be focus on adding another tables from <a href="https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_012_health_indicators_BRFSS2015.csv">Kaggle</a> that can be used complementary to the one used here.</p>

<p>A important information from our analysis is the indicative that the features <strong>BMI</strong>, <strong>GenHlth</strong>, <strong>Age</strong>, <strong>HighBP</strong>, and <strong>HighChol</strong> are the most predictive risk factors for diabetes, consistent with established clinical insights. Our project could reduce the feature space from 22 possible risk factors to just a subset of five, which simplified the predictive model without compromising accuracy.</p>

<p>The Random Forest model, using these five risk factors, proved to be the most balanced in terms of complexity and predictive capacity, and was selected for the final model deployment.</p>


  
    
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-marcosbenicio-github-io-1.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



</div>


    </div>

  </body>
</html>
